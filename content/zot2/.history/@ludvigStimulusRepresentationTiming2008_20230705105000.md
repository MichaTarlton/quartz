---

aliases: ["NA et al. (2008)",]
aliases: ["Ludvig et al. (2008)"]
type: citation
status: open
project: NA

priority: P5

creationtag: <% tp.file.creation_date() %>
priority: P2
creationtag: 2023-05-26 14:39
people: ["E.A. Ludvig", "R.S. Sutton", "E.J. Kehoe"]
title: "Stimulus Representation and the Timing of Reward-Prediction Errors in Models of the Dopamine System"
dateadd: 2023-05-26T12:37:53Z
citetype: Journal Article
year: 2008
journal: Neural Computation
URL: "NA"
DOI: "10.1162/neco.2008.11-07-654"
citekey: ludvigStimulusRepresentationTiming2008
collection: Timing Models
tags: [NA]
---

# Stimulus Representation and the Timing of Reward-Prediction Errors in Models of the Dopamine System
Read:: 
- [ ] Stimulus Representation and the Timing of Reward-Prediction Errors in Models of the Dopamine System E.A. Ludvig, R.S. Sutton, E.J. Kehoe 2008 🛫 2023-05-26 #reading #citation
Print::  ❌
Zotero Link:: [Ludvig et al. - 2008 - Stimulus Representation and the Timing of Reward-P.pdf](zotero://open-pdf/library/items/IMZUYMG7)
PDF:: NA
Files:: [Ludvig et al. - 2008 - Stimulus Representation and the Timing of Reward-P.pdf](file:///C:%5CUsers%5Cmichaelt%5CInsync%5Cm@tarlton.info%5CGoogle%20Drive%5C06.%20Zotero%5Cstorage%5CIMZUYMG7%5CLudvig%20et%20al.%20-%202008%20-%20Stimulus%20Representation%20and%20the%20Timing%20of%20Reward-P.pdf)
Reading Note:: 
Web Rip:: 

```dataview
TABLE without id
file.link as "Related Files",
title as "Title",
type as "type"
FROM "" AND -"ZZ. planning"
WHERE citekey = "ludvigStimulusRepresentationTiming2008" 
SORT file.cday DESC
```


> [!Excerpt] Abstract
> The phasic firing of dopamine neurons has been theorized to encode a reward-prediction error as formalized by the temporal-difference (TD) algorithm in reinforcement learning. Most TD models of dopamine have assumed a stimulus representation, known as the complete serial compound, in which each moment in a trial is distinctly represented. We introduce a more realistic temporal stimulus representation for the TD model. In our model, all external stimuli, including rewards, spawn a series of internal microstimuli, which grow weaker and more diffuse over time. These microstimuli are used by the TD learning algorithm to generate predictions of future reward. This new stimulus representation injects temporal generalization into the TD model and enhances correspondence between model and data in several experiments, including those when rewards are omitted or received early. This improved fit mostly derives from the absence of large negative errors in the new model, suggesting that dopamine alone can encode the full range of TD errors in these situations.


# Quick Reference

# Top Comments
Let's say grey is for overall comments
 

# Tasks

# Topics


# Further Reading 
 

----
# Notes


----
# Extracted Annotations and Comments


# Figures