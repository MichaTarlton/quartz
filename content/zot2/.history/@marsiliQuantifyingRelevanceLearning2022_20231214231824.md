

# Quantifying Relevance in Learning and Inference
Read:: 
- [x] Quantifying Relevance in Learning and Inference M. Marsili, Y. Roudi 2022 #reading #citation 🛫 2023-01-19 ✅ 2023-01-19
Print::  ❌
Zotero Link:: NA
PDF:: NA
Files:: [arXiv.org Snapshot](file:///C:%5CUsers%5Cmichaelt%5CZotero%5Cstorage%5C7IRHFTMA%5C2202.html); [Marsili and Roudi - 2022 - Quantifying Relevance in Learning and Inference.pdf](file:///C:%5CUsers%5Cmichaelt%5CZotero%5Cstorage%5C9QEUGLR4%5CMarsili%20and%20Roudi%20-%202022%20-%20Quantifying%20Relevance%20in%20Learning%20and%20Inference.pdf)
Reading Note:: [[M. Marsili, Y. Roudi (2022)]]
Web Rip:: 

```dataview
TABLE without id
file.link as "Related Files",
title as "Title",
type as "type"
FROM "" AND -"ZZ. planning"
WHERE citekey = "marsiliQuantifyingRelevanceLearning2022" 
SORT file.cday DESC
```


> [!Excerpt] Abstract
> Learning is a distinctive feature of intelligent behaviour. High-throughput experimental data and Big Data promise to open new windows on complex systems such as cells, the brain or our societies. Yet, the puzzling success of Artificial Intelligence and Machine Learning shows that we still have a poor conceptual understanding of learning. These applications push statistical inference into uncharted territories where data is high-dimensional and scarce, and prior information on "true" models is scant if not totally absent. Here we review recent progress on understanding learning, based on the notion of "relevance". The relevance, as we define it here, quantifies the amount of information that a dataset or the internal representation of a learning machine contains on the generative model of the data. This allows us to define maximally informative samples, on one hand, and optimal learning machines on the other. These are ideal limits of samples and of machines, that contain the maximal amount of information about the unknown generative process, at a given resolution (or level of compression). Both ideal limits exhibit critical features in the statistical sense: Maximally informative samples are characterised by a power-law frequency distribution (statistical criticality) and optimal learning machines by an anomalously large susceptibility. The trade-off between resolution (i.e. compression) and relevance distinguishes the regime of noisy representations from that of lossy compression. These are separated by a special point characterised by Zipf's law statistics. This identifies samples obeying Zipf's law as the most compressed loss-less representations that are optimal in the sense of maximal relevance. Criticality in optimal learning machines manifests in an exponential degeneracy of energy levels, that leads to unusual thermodynamic properties.


# Quick Reference

# Top Comments

# Tasks

# Topics


# Further Reading 
 

--
# Extracted Annotations and Comments


# Figures