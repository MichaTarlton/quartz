---

aliases: ["NA et al. (2022)",]
aliases:
  - Cramer et al. (2022)
type: citation
status: open
project: NA
priority: P5

creationtag: <% tp.file.creation_date() %>

people: ["B. Cramer", "S. Billaudelle", "S. Kanya", "A. Leibfried", "A. Grübl", "V. Karasenko", "C. Pehle", "K. Schreiber", "Y. Stradmann", "J. Weis", "J. Schemmel", "F. Zenke"]
creationtag: 2022-12-01 14:56
people:
  - B. Cramer
  - S. Billaudelle
  - S. Kanya
  - A. Leibfried
  - A. Grübl
  - V. Karasenko
  - C. Pehle
  - K. Schreiber
  - Y. Stradmann
  - J. Weis
  - J. Schemmel
  - F. Zenke
title: Surrogate gradients for analog neuromorphic computing
dateadd: 2022-12-01T13:51:35Z
citetype: Journal Article
year: 2022
journal: Proceedings of the National Academy of Sciences of the United States of America

URL: "NA"

DOI: "10.1073/pnas.2109194119"
URL: NA
DOI: 10.1073/pnas.2109194119
citekey: cramerSurrogateGradientsAnalog2022

collection: Notion

tags: [Action Potentials, Algorithms, Brain, Computers, Models, Biological, Models, Neurological, Models, Theoretical, Neural Networks, Computer, Neurons, neuromorphic hardware, notion, recurrent neural networks, self-calibration, spiking neural networks, surrogate gradients]
collection: NA
tags:
  - Action Potentials
  - Algorithms
  - Brain
  - Computers
  - Models
  - Biological
  - Models
  - Neurological
  - Models
  - Theoretical
  - Neural Networks
  - Computer
  - Neurons
  - neuromorphic hardware
  - recurrent neural networks
  - self-calibration
  - spiking neural networks
  - surrogate gradients
file: ""
---

# Surrogate gradients for analog neuromorphic computing
Read:: 
- [ ] Surrogate gradients for analog neuromorphic computing B. Cramer, S. Billaudelle, S. Kanya, A. Leibfried, A. Grübl, V. Karasenko, C. Pehle, K. Schreiber, Y. Stradmann, J. Weis, J. Schemmel, F. Zenke 2022 🛫 2022-12-01 #reading #citation
Print::  ❌

Zotero Link:: [Cramer et al_2022_Surrogate gradients for analog neuromorphic computing.pdf](zotero://open-pdf/library/items/UAYTU6SE); [Notion](); [pnas.2109194119.sapp.pdf](zotero://open-pdf/library/items/ZBSQKEIJ); [PubMed entry]()
Zotero Link:: NA
PDF:: NA

Files:: [Cramer et al_2022_Surrogate gradients for analog neuromorphic computing.pdf](file:///C:%5CUsers%5Cmichaelt%5CInsync%5Cm@tarlton.info%5CGoogle%20Drive%5C06.%20Zotero%5Cstorage%5CUAYTU6SE%5CCramer%20et%20al_2022_Surrogate%20gradients%20for%20analog%20neuromorphic%20computing.pdf); [Notion](file:///); [pnas.2109194119.sapp.pdf](file:///C:%5CUsers%5Cmichaelt%5CInsync%5Cm@tarlton.info%5CGoogle%20Drive%5C06.%20Zotero%5Cstorage%5CZBSQKEIJ%5Cpnas.2109194119.sapp.pdf); [PubMed entry](file:///)
Files:: [Cramer et al_2022_Surrogate gradients for analog neuromorphic computing.pdf](file:///C:%5CUsers%5Cmichaelt%5CZotero%5Cstorage%5CUAYTU6SE%5CCramer%20et%20al_2022_Surrogate%20gradients%20for%20analog%20neuromorphic%20computing.pdf); [PubMed entry](file:///)
Reading Note:: [[B. Cramer, S. Billaudelle, S. Kanya, A. Leibfried, A. Grübl, V. Karasenko, C. Pehle, K. Schreiber, Y. Stradmann, J. Weis, J. Schemmel, F. Zenke (2022)]]
Web Rip:: 

```dataview
TABLE without id
file.link as "Related Files",
title as "Title",
type as "type"
FROM "" AND -"ZZ. planning"
WHERE citekey = "cramerSurrogateGradientsAnalog2022" 
SORT file.cday DESC
```


> [!Excerpt] Abstract
> To rapidly process temporal information at a low metabolic cost, biological neurons integrate inputs as an analog sum, but communicate with spikes, binary events in time. Analog neuromorphic hardware uses the same principles to emulate spiking neural networks with exceptional energy efficiency. However, instantiating high-performing spiking networks on such hardware remains a significant challenge due to device mismatch and the lack of efficient training algorithms. Surrogate gradient learning has emerged as a promising training strategy for spiking networks, but its applicability for analog neuromorphic systems has not been demonstrated. Here, we demonstrate surrogate gradient learning on the BrainScaleS-2 analog neuromorphic system using an in-the-loop approach. We show that learning self-corrects for device mismatch, resulting in competitive spiking network performance on both vision and speech benchmarks. Our networks display sparse spiking activity with, on average, less than one spike per hidden neuron and input, perform inference at rates of up to 85,000 frames per second, and consume less than 200 mW. In summary, our work sets several benchmarks for low-energy spiking network processing on analog neuromorphic hardware and paves the way for future on-chip learning algorithms.


# Quick Reference

# Top Comments

Let's say grey is for overall comments

# Tasks

# Topics


# Further Reading 
 

----
# Notes


----
# Extracted Annotations and Comments


# Figures
## Table 2
![[@cramerSurrogateGradientsAnalog2022-image-20230817150247225.png]]

