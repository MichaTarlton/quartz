---
aliases:
  - Bojian Yin, Federico Corradi, Sander M. Bohté 2020
type: citation
status: open
creationtag: 2022-06-17 12:14
people:
  - Bojian Yin
  - Federico Corradi
  - Sander M. Bohté
title: Effective and Efficient Computation with Multiple-timescale Spiking
  Recurrent Neural Networks
dateadd: 2022-05-20T14:51:53Z
citetype: conferencePaper
year: 2020
journal: International Conference on Neuromorphic Systems 2020
URL: NA
DOI: 10.1145/3407197.3407225
citekey: yinEffectiveEfficientComputation2020
collection: SNN - DL
tags:
  - backpropagation through time
  - spiking neural networks
  - surrogate gradient
file: ""
---

# Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks
Read:: 
Project:: []
Print::  ❌
- [ ] print 
Zotero Link:: NA
PDF:: NA
Files:: [Yin et al_2020_Effective and Efficient Computation with Multiple-timescale Spiking Recurrent.pdf](file:///home/michaelt/Insync/m@tarlton.info/Google%20Drive/06.%20Zotero/storage/TDSZPKZR/Yin%20et%20al_2020_Effective%20and%20Efficient%20Computation%20with%20Multiple-timescale%20Spiking%20Recurrent.pdf)
Reading Note:: [[Bojian Yin, Federico Corradi, Sander M. Bohté 2020]]

# Abstract
The emergence of brain-inspired neuromorphic computing as a paradigm for edge AI is motivating the search for high-performance and efficient spiking neural networks to run on this hardware. However, compared to classical neural networks in deep learning, current spiking neural networks lack competitive performance in compelling areas. Here, for sequential and streaming tasks, we demonstrate how a novel type of adaptive spiking recurrent neural network (SRNN) is able to achieve state-of-the-art performance compared to other spiking neural networks and almost reach or exceed the performance of classical recurrent neural networks (RNNs) while exhibiting sparse activity. From this, we calculate a > 100x energy improvement for our SRNNs over classical RNNs on the harder tasks. To achieve this, we model standard and adaptive multiple-timescale spiking neurons as self-recurrent neural units, and leverage surrogate gradients and auto-differentiation in the PyTorch Deep Learning framework to efficiently implement backpropagation-through-time, including learning of the important spiking neuron parameters to adapt our spiking neurons to the tasks.

# Quick Reference


# Top Comments


# Topics


# Tasks


----
# Notes


----
# Extracted Annotations and Comments