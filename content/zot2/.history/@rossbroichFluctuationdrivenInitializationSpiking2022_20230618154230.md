---
aliases:
  - Julian Rossbroich, Julia Gygax, Friedemann Zenke 2022
type: citation
status: open
project: null
priority: null
creationtag: 2022-06-23 10:51
people:
  - Julian Rossbroich
  - Julia Gygax
  - Friedemann Zenke
title: Fluctuation-driven initialization for spiking neural network training
dateadd: 2022-06-23T08:50:23Z
citetype: preprint
year: 2022
journal: NA
URL: NA
DOI: 10.48550/arXiv.2206.10226
citekey: rossbroichFluctuationdrivenInitializationSpiking2022
collection: SNN - DL
tags:
  - Computer Science - Neural and Evolutionary Computing
  - Quantitative Biology - Neurons and Cognition
  - SNN
  - plasticity
file: ""
---

# Fluctuation-driven initialization for spiking neural network training
Read:: 
Project:: []
Print::  ‚ùå
- [ ] print 
Zotero Link:: NA
PDF:: NA
Files:: [arXiv.org Snapshot](file:///home/michaelt/Insync/m@tarlton.info/Google%20Drive/06.%20Zotero/storage/GCMFXI9C/2206.html); [Rossbroich et al_2022_Fluctuation-driven initialization for spiking neural network training.pdf](file:///home/michaelt/Insync/m@tarlton.info/Google%20Drive/06.%20Zotero/storage/24WN6Z9E/Rossbroich%20et%20al_2022_Fluctuation-driven%20initialization%20for%20spiking%20neural%20network%20training.pdf)
Reading Note:: [[Julian Rossbroich, Julia Gygax, Friedemann Zenke 2022]]
Web Rip:: 
```dataview
TABLE without id
file.link as "Related Files",
title as "Title",
type as "type"
FROM "" AND -"ZZ. planning"
WHERE contains(citekey, "rossbroichFluctuationdrivenInitializationSpiking2022")
SORT file.cday DESC
```

# Abstract
Spiking neural networks (SNNs) underlie low-power, fault-tolerant information processing in the brain and could constitute a power-efficient alternative to conventional deep neural networks when implemented on suitable neuromorphic hardware accelerators. 

However, instantiating SNNs that solve complex computational tasks in-silico remains a significant challenge. Surrogate gradient (SG) techniques have emerged as a standard solution for training SNNs end-to-end. Still, their success depends on synaptic weight initialization, similar to conventional artificial neural networks (ANNs). 

Yet, unlike in the case of ANNs, it remains elusive what constitutes a good initial state for an SNN. Here, we develop a general initialization strategy for SNNs inspired by the fluctuation-driven regime commonly observed in the brain.

Specifically, we derive practical solutions for data-dependent weight initialization that ensure fluctuation-driven firing in the widely used leaky integrate-and-fire (LIF) neurons. We empirically show that SNNs initialized following our strategy exhibit superior learning performance when trained with SGs. 

These findings generalize across several datasets and SNN architectures, including fully connected, deep convolutional, recurrent, and more biologically plausible SNNs obeying Dale's law. Thus fluctuation-driven initialization provides a practical, versatile, and easy-to-implement strategy for improving SNN training performance on diverse tasks in neuromorphic engineering and computational neuroscience.

# Quick Reference


# Top Comments


# Topics


# Tasks


----
# Notes
Comment: 30 pages, 7 figures, plus supplementary material

----
# Extracted Annotations and Comments