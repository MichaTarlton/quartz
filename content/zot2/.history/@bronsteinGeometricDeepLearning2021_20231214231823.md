

# Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges
Read:: 
- [ ] Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges M.M. Bronstein, J. Bruna, T. Cohen, P. Veličković 2021 🛫 NA #reading #citation
Print::  ❌
Zotero Link:: NA
PDF:: NA
Files:: [arXiv.org Snapshot](file:////home/michaelt/Insync/m@tarlton.info/Google%20Drive/06.%20Zotero/storage/YFGDAL53/2104.html); [Bronstein et al_2021_Geometric Deep Learning.pdf](file:////home/michaelt/Insync/m@tarlton.info/Google%20Drive/06.%20Zotero/storage/UCXEX5LG/Bronstein%20et%20al_2021_Geometric%20Deep%20Learning.pdf)

Files:: [arXiv.org Snapshot](file:///C:%5CUsers%5Cmichaelt%5CInsync%5Cm@tarlton.info%5CGoogle%20Drive%5C06.%20Zotero%5Cstorage%5CYFGDAL53%5C2104.html); [Bronstein et al_2021_Geometric Deep Learning.pdf](file:///C:%5CUsers%5Cmichaelt%5CInsync%5Cm@tarlton.info%5CGoogle%20Drive%5C06.%20Zotero%5Cstorage_new%5CarXiv_2021%5CBronstein%20et%20al_2021_Geometric%20Deep%20Learning.pdf)
Reading Note:: [[M.M. Bronstein, J. Bruna, T. Cohen, P. Veličković (2021)]]
Web Rip:: 

```dataview
TABLE without id
file.link as "Related Files",
title as "Title",
type as "type"
FROM "" AND -"ZZ. planning"
WHERE citekey = "bronsteinGeometricDeepLearning2021" 
SORT file.cday DESC

> [!Excerpt] Abstract
```

# Abstract
The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.

# Quick Reference


# Top Comments

Let's say grey is for overall comments


# Topics


# Further Reading 
 

--
# Extracted Annotations and Comments


# Figures