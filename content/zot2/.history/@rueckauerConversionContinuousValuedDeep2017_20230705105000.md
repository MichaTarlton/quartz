---

aliases: ["NA et al. (2017)",]
aliases:
  - B. Rueckauer, I. Lungu, Y. Hu, M. Pfeiffer, S. Liu (2017)
type: citation
status: open
project: NA
priority: P5

creationtag: <% tp.file.creation_date() %>

people: ["B. Rueckauer", "I. Lungu", "Y. Hu", "M. Pfeiffer", "S. Liu"]
creationtag: 2023-02-28 14:47
people:
  - B. Rueckauer
  - I. Lungu
  - Y. Hu
  - M. Pfeiffer
  - S. Liu
title: Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven
  Networks for Image Classification
dateadd: 2023-02-28T13:46:27Z
citetype: Journal Article
year: 2017
journal: Frontiers in Neuroscience
URL: NA
DOI: NA
citekey: rueckauerConversionContinuousValuedDeep2017
collection: NA
tags:
  - NA
file: ""
---

# Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification

URL: "NA"

DOI: "NA"

tags: [NA]
Read:: 
- [ ] Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification B. Rueckauer, I. Lungu, Y. Hu, M. Pfeiffer, S. Liu 2017 🛫 2023-02-28 #reading #citation
Print::  ❌
Zotero Link:: NA
PDF:: NA
Files:: [Rueckauer et al_2017_Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven.pdf](file:///C:%5CUsers%5Cmichaelt%5CInsync%5Cm@tarlton.info%5CGoogle%20Drive%5C06.%20Zotero%5Cstorage_new%5CFrontiers%20in%20Neuroscience_2017%5CRueckauer%20et%20al_2017_Conversion%20of%20Continuous-Valued%20Deep%20Networks%20to%20Efficient%20Event-Driven.pdf)
Reading Note:: [[B. Rueckauer, I. Lungu, Y. Hu, M. Pfeiffer, S. Liu (2017)]]
Web Rip:: 

```dataview
TABLE without id
file.link as "Related Files",
title as "Title",
type as "type"
FROM "" AND -"ZZ. planning"
WHERE citekey = "rueckauerConversionContinuousValuedDeep2017" 
SORT file.cday DESC
```


> [!Excerpt] Abstract
> Spiking neural networks (SNNs) can potentially offer an efficient way of doing inference because the neurons in the networks are sparsely activated and computations are event-driven. Previous work showed that simple continuous-valued deep Convolutional Neural Networks (CNNs) can be converted into accurate spiking equivalents. These networks did not include certain common operations such as max-pooling, softmax, batch-normalization and Inception-modules. This paper presents spiking equivalents of these operations therefore allowing conversion of nearly arbitrary CNN architectures. We show conversion of popular CNN architectures, including VGG-16 and Inception-v3, into SNNs that produce the best results reported to date on MNIST, CIFAR-10 and the challenging ImageNet dataset. SNNs can trade off classification error rate against the number of available operations whereas deep continuous-valued neural networks require a fixed number of operations to achieve their classification error rate. From the examples of LeNet for MNIST and BinaryNet for CIFAR-10, we show that with an increase in error rate of a few percentage points, the SNNs can achieve more than 2x reductions in operations compared to the original CNNs. This highlights the potential of SNNs in particular when deployed on power-efficient neuromorphic spiking neuron chips, for use in embedded applications.


# Quick Reference

# Top Comments

Let's say grey is for overall comments

# Tasks

# Topics


# Further Reading 
 

----
# Notes


----
# Extracted Annotations and Comments


# Figures