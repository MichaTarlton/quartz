---
aliases: ["Bose et al. (2019)",]
type: citation
status: open
project: NA
priority: P5

creationtag: 2023-06-21 17:21
creationtag: 2023-06-21 17:21
people: ["A. Bose", "Ã. Byrne", "J. Rinzel"]
title: "A neuromechanistic model for rhythmic beat generation"
dateadd: 2023-06-21T15:06:08Z
citetype: Journal Article
year: 2019
journal: PLoS computational biology
URL: "NA"
DOI: "10.1371/journal.pcbi.1006450"
citekey: boseNeuromechanisticModelRhythmic2019
collection: 00 SBFA paper
tags: [Acoustic Stimulation, Auditory Perception, Biomechanical Phenomena, Brain, Electroencephalography, Gamma Rhythm, Humans, Models, Neurological, Music, Neurons, Periodicity, Time Perception]
---

# A neuromechanistic model for rhythmic beat generation
Read:: 
- [ ] A neuromechanistic model for rhythmic beat generation A. Bose, Ã. Byrne, J. Rinzel 2019 ðŸ›« 2023-06-21 !!2 #citation #reading [link](https://todoist.com/showTask?id=6985399118) #todoist %%[todoist_id:: 6985399118]%%
Print::  âŒ
Zotero Link:: [Bose et al_2019_A neuromechanistic model for rhythmic beat generation.pdf](zotero://open-pdf/library/items/X7I95562); [PubMed entry]()
PDF:: NA
Files:: [Bose et al_2019_A neuromechanistic model for rhythmic beat generation.pdf](file:///C:%5CUsers%5Cmichaelt%5CInsync%5Cm@tarlton.info%5CGoogle%20Drive%5C06.%20Zotero%5Cstorage_new%5CPLoS%20computational%20biology_2019%5CBose%20et%20al_2019_A%20neuromechanistic%20model%20for%20rhythmic%20beat%20generation.pdf); [PubMed entry](file:///)
Reading Note:: 
Web Rip:: 

```dataview
TABLE without id
file.link as "Related Files",
title as "Title",
type as "type"
FROM "" AND -"ZZ. planning"
WHERE citekey = "boseNeuromechanisticModelRhythmic2019" 
SORT file.cday DESC
```


> [!Excerpt] Abstract
> When listening to music, humans can easily identify and move to the beat. Numerous experimental studies have identified brain regions that may be involved with beat perception and representation. Several theoretical and algorithmic approaches have been proposed to account for this ability. Related to, but different from the issue of how we perceive a beat, is the question of how we learn to generate and hold a beat. In this paper, we introduce a neuronal framework for a beat generator that is capable of learning isochronous rhythms over a range of frequencies that are relevant to music and speech. Our approach combines ideas from error-correction and entrainment models to investigate the dynamics of how a biophysically-based neuronal network model synchronizes its period and phase to match that of an external stimulus. The model makes novel use of on-going faster gamma rhythms to form a set of discrete clocks that provide estimates, but not exact information, of how well the beat generator spike times match those of a stimulus sequence. The beat generator is endowed with plasticity allowing it to quickly learn and thereby adjust its spike times to achieve synchronization. Our model makes generalizable predictions about the existence of asymmetries in the synchronization process, as well as specific predictions about resynchronization times after changes in stimulus tempo or phase. Analysis of the model demonstrates that accurate rhythmic time keeping can be achieved over a range of frequencies relevant to music, in a manner that is robust to changes in parameters and to the presence of noise.


# Quick Reference

# Top Comments

Let's say grey is for overall comments
- Presents a framework but is not directly SBF

# Tasks

# Topics


# Further Reading 
 

----
# Notes


----
# Extracted Annotations and Comments


# Figures