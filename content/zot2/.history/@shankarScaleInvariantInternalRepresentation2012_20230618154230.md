---
aliases:
  - Karthik H. Shankar, Marc W. Howard 2012
type: citation
status: open
project: null
priority: null
creationtag: 2022-07-20 18:04
people:
  - Karthik H. Shankar
  - Marc W. Howard
title: A Scale-Invariant Internal Representation of Time
dateadd: 2022-07-20T16:01:22Z
citetype: journalArticle
year: 2012
journal: Neural Computation
URL: NA
DOI: 10.1162/NECO_a_00212
citekey: shankarScaleInvariantInternalRepresentation2012
collection: Off of Petter 2018
tags:
  - NA
file: ""
---

# A Scale-Invariant Internal Representation of Time
Read:: 
Print::  ‚ùå
Zotero Link:: NA
PDF:: NA
Files:: [Shankar_Howard_2012_A Scale-Invariant Internal Representation of Time.pdf](file:///home/michaelt/Insync/m@tarlton.info/Google%20Drive/06.%20Zotero/storage/IUNY5KE6/Shankar_Howard_2012_A%20Scale-Invariant%20Internal%20Representation%20of%20Time.pdf); [Snapshot](file:///home/michaelt/Insync/m@tarlton.info/Google%20Drive/06.%20Zotero/storage/JKF7XHXI/A-Scale-Invariant-Internal-Representation-of-Time.html)
Reading Note:: [[Karthik H. Shankar, Marc W. Howard 2012]]
Web Rip:: 

# Abstract
We propose a principled way to construct an internal representation of the
temporal stimulus history leading up to the present moment. A set of leaky
integrators performs a Laplace transform on the stimulus function, and a linear
operator approximates the inversion of the Laplace transform. The result is a
representation of stimulus history that retains information about the temporal
sequence of stimuli. This procedure naturally represents more recent stimuli
more accurately than less recent stimuli; the decrement in accuracy is precisely
scale invariant. This procedure also yields time cells that fire at specific
latencies following the stimulus with a scale-invariant temporal spread.
Combined with a simple associative memory, this representation gives rise to a
moment-to-moment prediction that is also scale invariant in time. We propose
that this scale-invariant representation of temporal stimulus history could
serve as an underlying representation accessible to higher-level behavioral and
cognitive mechanisms. In order to illustrate the potential utility of this
scale-invariant representation in a variety of fields, we sketch applications
using minimal performance functions to problems in classical conditioning,
interval timing, scale-invariant learning in autoshaping, and the persistence of
the recency effect in episodic memory across timescales.

# Quick Reference


# Top Comments


# Topics


# Tasks


----
# Notes


----
# Extracted Annotations and Comments