---
aliases:
  - Aaron Russell Voelker 2019
type: citation
status: open
people:
  - Aaron Russell Voelker
title: Dynamical Systems in Spiking Neuromorphic Hardware
dateadd: 2022-05-08T14:37:08Z
citetype: journalArticle
year: 2019
journal: NA
URL: NA
DOI: NA
citekey: voelkerDynamicalSystemsSpiking2019
collection: Interval Timing, Legendre Memory Units
tags:
  - Interval Timing
  - Legendre Memory Units
  - neuromorphic
file: ""
---

# Dynamical Systems in Spiking Neuromorphic Hardware
Read:: 
Project:: []
Print::  ❌
- [ ] print 
Zotero Link:: NA
PDF:: NA
Files:: [Snapshot](file:///home/michaelt/Insync/m@tarlton.info/Google%20Drive/06.%20Zotero/storage/F7YAT2JC/14625.html); [Voelker_2019_Dynamical Systems in Spiking Neuromorphic Hardware.pdf](file:///home/michaelt/Insync/m@tarlton.info/Google%20Drive/06.%20Zotero/storage/3UHWEJC2/Voelker_2019_Dynamical%20Systems%20in%20Spiking%20Neuromorphic%20Hardware.pdf)
Reading Note:: [[Aaron Russell Voelker 2019]]

# Abstract
Dynamical systems are universal computers. They can perceive stimuli, remember, learn from feedback, plan sequences of actions, and coordinate complex behavioural responses. The Neural Engineering Framework (NEF) provides a general recipe to formulate models of such systems as coupled sets of nonlinear differential equations and compile them onto recurrently connected spiking neural networks – akin to a programming language for spiking models of computation. The Nengo software ecosystem supports the NEF and compiles such models onto neuromorphic hardware. In this thesis, we analyze the theory driving the success of the NEF, and expose several core principles underpinning its correctness, scalability, completeness, robustness, and extensibility. We also derive novel theoretical extensions to the framework that enable it to far more effectively leverage a wide variety of dynamics in digital hardware, and to exploit the device-level physics in analog hardware. At the same time, we propose a novel set of spiking algorithms that recruit an optimal nonlinear encoding of time, which we call the Delay Network (DN). Backpropagation across stacked layers of DNs dramatically outperforms stacked Long Short-Term Memory (LSTM) networks—a state-of-the-art deep recurrent architecture—in accuracy and training time, on a continuous-time memory task, and a chaotic time-series prediction benchmark. The basic component of this network is shown to function on state-of-the-art spiking neuromorphic hardware including Braindrop and Loihi. This implementation approaches the energy-efficiency of the human brain in the former case, and the precision of conventional computation in the latter case.

# Comments


# Topics


# Tasks


----
# Notes



----
# Extracted Annotations