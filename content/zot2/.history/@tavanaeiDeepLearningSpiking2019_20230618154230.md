---
aliases:
  - A. Tavanaei, M. Ghodrati, S.R. Kheradpisheh, T. Masquelier, A. Maida (2019)
type: citation
status: open
project: NA
priority: P5
creationtag: 2023-02-23 17:11
people:
  - A. Tavanaei
  - M. Ghodrati
  - S.R. Kheradpisheh
  - T. Masquelier
  - A. Maida
title: Deep learning in spiking neural networks
dateadd: 2022-05-13T11:42:16Z
citetype: Journal Article
year: 2019
journal: Neural Networks
URL: NA
DOI: 10.1016/j.neunet.2018.12.002
citekey: tavanaeiDeepLearningSpiking2019
collection: SNN - DL
tags:
  - Biological plausibility
  - Deep learning
  - Machine learning
  - Power-efficient architecture
  - Spiking neural network
file: ""
---

# Deep learning in spiking neural networks
Read:: 
- [ ] Deep learning in spiking neural networks A. Tavanaei, M. Ghodrati, S.R. Kheradpisheh, T. Masquelier, A. Maida 2019 🛫 2023-02-23 #reading #citation
Print::  ❌
Zotero Link:: NA
PDF:: NA
Files:: [ScienceDirect Snapshot](file:///C:%5CUsers%5Cmichaelt%5CInsync%5Cm@tarlton.info%5CGoogle%20Drive%5C06.%20Zotero%5Cstorage%5CEG6AVETB%5CS0893608018303332.html); [Tavanaei et al_2019_Deep learning in spiking neural networks.pdf](file:///C:%5CUsers%5Cmichaelt%5CInsync%5Cm@tarlton.info%5CGoogle%20Drive%5C06.%20Zotero%5Cstorage%5CJA8J5RRY%5CTavanaei%20et%20al_2019_Deep%20learning%20in%20spiking%20neural%20networks.pdf)
Reading Note:: [[A. Tavanaei, M. Ghodrati, S.R. Kheradpisheh, T. Masquelier, A. Maida (2019)]]
Web Rip:: 

```dataview
TABLE without id
file.link as "Related Files",
title as "Title",
type as "type"
FROM "" AND -"ZZ. planning"
WHERE citekey = "tavanaeiDeepLearningSpiking2019" 
SORT file.cday DESC
```


> [!Excerpt] Abstract
> In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons’ transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.


# Quick Reference

# Top Comments

# Tasks

# Topics


# Further Reading 
 

----
# Notes


----
# Extracted Annotations and Comments


# Figures