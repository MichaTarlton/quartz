---
zotero-key: T8MRKPRH
zt-attachments:
  - "5045"
title: Scaling Laws for Neural Language Models
citekey: kaplanScalingLawsNeural2020
aliases:
  - Kaplan et al. (2020)
people:
  - Jared Kaplan
  - Sam McCandlish
  - Tom Henighan
  - Tom B. Brown
  - Benjamin Chess
  - Rewon Child
  - Scott Gray
  - Alec Radford
  - Jeffrey Wu
  - Dario Amodei
dateadd: 2023-11-20T14:29:09.000Z
citetype: preprint
year: 2020
DOI: 10.48550/arXiv.2001.08361
tags:
  - Computer Science - Machine Learning
  - Statistics - Machine Learning
type: citation
status: open
project: NA
priority: P5
creationtag: 2023-11-29 15:24
---
# Scaling Laws for Neural Language Models
Read:: - [ ] Kaplan et al. (2020) - Scaling Laws for Neural Language Models üõ´2023-11-29 !!2 #rd #citation #todoist
Print:: ¬†‚ùå
Zotero Link:: [Zotero](zotero://select/library/items/T8MRKPRH) 
Files:: [attachment](<file:///C:/Users/michaelt/Insync/m@tarlton.info/Google%20Drive/06.%20Zotero/storage_new/arXiv_2020/Kaplan%20et%20al_2020_Scaling%20Laws%20for%20Neural%20Language%20Models.pdf>)
Reading Note::
Web Rip::
url:: http://arxiv.org/abs/2001.08361

```dataview
TABLE without id
file.link as "Related Files",
title as "Title",
type as "type"
FROM "" AND -"ZZ. planning"
WHERE citekey = "kaplanScalingLawsNeural2020" 
SORT file.cday DESC
```

> [!Excerpt] Abstract
> We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.
# Quick Reference

# Top Notes

# Tasks






















