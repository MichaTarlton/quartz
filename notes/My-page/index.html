<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="[[06.07.23]] New #neuromorphics journal? Some guy posted this JoVE | Peer Reviewed Scientific Video Journal - Methods and Protocols in the SNUFA discord."><meta property="og:title" content="Cool new page"><meta property="og:description" content="[[06.07.23]] New #neuromorphics journal? Some guy posted this JoVE | Peer Reviewed Scientific Video Journal - Methods and Protocols in the SNUFA discord."><meta property="og:type" content="website"><meta property="og:image" content="https://quartz.tarlton.info/icon.png"><meta property="og:url" content="https://quartz.tarlton.info/notes/My-page/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Cool new page"><meta name=twitter:description content="[[06.07.23]] New #neuromorphics journal? Some guy posted this JoVE | Peer Reviewed Scientific Video Journal - Methods and Protocols in the SNUFA discord."><meta name=twitter:image content="https://quartz.tarlton.info/icon.png"><meta name=twitter:site content="michatarlton"><title>Cool new page</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://quartz.tarlton.info//icon.png><link href=https://quartz.tarlton.info/styles.19109a40042e9f0e72e952fda4442a34.min.css rel=stylesheet><link href=https://quartz.tarlton.info/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://quartz.tarlton.info/js/darkmode.7c50dfd080feb51c14784bf866b085a2.min.js></script>
<script src=https://quartz.tarlton.info/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://quartz.tarlton.info/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://quartz.tarlton.info/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://quartz.tarlton.info/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://quartz.tarlton.info/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://quartz.tarlton.info/",fetchData=Promise.all([fetch("https://quartz.tarlton.info/indices/linkIndex.c2d1c5fb022ed186d13e28d8e7cf68aa.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://quartz.tarlton.info/indices/contentIndex.b60914a68cf037a34e72ed3ac3bac4b2.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://quartz.tarlton.info",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://quartz.tarlton.info",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/quartz.tarlton.info\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=quartz.tarlton.info src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://quartz.tarlton.info/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://quartz.tarlton.info/>🪴 Quartz 3.3</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Cool new page</h1><p class=meta>Last updated
Jul 25, 2023
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/notes/My%20page.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://quartz.tarlton.info/tags/SBF-writing/>Sbf, writing</a></li></ul><a href=#060723><h1 id=060723><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">06.07.23</a></h1></a><a href=#new-neuromorphics-journal><h2 id=new-neuromorphics-journal><span class=hanchor arialabel=Anchor># </span>New #neuromorphics journal?</h2></a><p>Some guy posted this
<a href=https://app.jove.com/methods-collections/2680 rel=noopener>JoVE | Peer Reviewed Scientific Video Journal - Methods and Protocols</a> in the SNUFA discord.</p><p>Mentioned that they are attaching a media element as well</p><a href=#notes-from-insomnia><h2 id=notes-from-insomnia><span class=hanchor arialabel=Anchor># </span>Notes from Insomnia</h2></a><p>Looks like a good flow for the presentation</p><a href=#neural-oscillations><h3 id=neural-oscillations><span class=hanchor arialabel=Anchor># </span>Neural Oscillations</h3></a><ul><li>Sub to Suprasecond range</li><li>Building supersecond cycling time cells from circuits of cells which fire in the microsecond range</li><li>Building towards to multi-second range and beyond is a key goal here</li><li>The only neural oscillation that is above one second is the <a class="internal-link broken">Infraslow</a> rhythms<ul><li>Don’t know much about these or how the generate</li></ul></li></ul><blockquote class=question-callout><p><strong>What intervals to time-cells cycle at?</strong>
Check <a class="internal-link broken">Mello et al. (2015)</a></p><ul><li>It says *“We found that neurons fired at delays spanning tens of seconds and that this pattern of responding reflected the interaction between time and the animals’ ongoing sensorimotor</li></ul><p><a class="internal-link broken">Neural representations of time, space and other continuous variables</a> has a slide showing up to 10 sec</p><ul><li>They cite a Mau et al. 2018</li></ul><p><a class="internal-link broken">Paton & Buonomano 2018</a></p><ul><li>You need to pull from this</li><li>Argues for distributed time encoding</li><li>See: <a class="internal-link broken">@patonNeuralBasisTiming2018</a>
Follow up to main question</li></ul></blockquote><a href=#striatal-beat-frequency><h3 id=striatal-beat-frequency><span class=hanchor arialabel=Anchor># </span>Striatal Beat Frequency</h3></a><ul><li>Model of coincidence detection and interval prediction</li><li>Very promising model</li><li>Recent variation (EIO) can produce the model with entrainment of theta and gamma waves<ul><li>better yet offers a method scaling (I think)</li></ul></li><li>Is this producing a time-cell?</li></ul><a href=#fixed-interval-task><h3 id=fixed-interval-task><span class=hanchor arialabel=Anchor># </span>Fixed-Interval Task</h3></a><ul><li>The point of the SBF is to reproduce animal behavior in a FIT (or Peak IT?)</li><li>This relies on mapping a “triggering” stimuli to restart the oscillators</li><li>However what and how does this occur?</li><li>Also relies on a “saving” of interval information to some “cold-storage”<ul><li>This may be different for the EIO</li></ul></li></ul><a href=#environmental-periodic-events-without-initiation-stimulus><h3 id=environmental-periodic-events-without-initiation-stimulus><span class=hanchor arialabel=Anchor># </span>Environmental Periodic events without initiation stimulus</h3></a><ul><li>Many events in a natural environment come without a stimulus</li><li>A non-biological example taken from automata theory is web-crawling<ul><li>Where webpages will update their content with some unknown stochastic frequency</li><li>An RL automata agent will need learn this periodicity and act on it accordingly</li></ul></li><li>Our model will learn for this</li></ul><a href=#what-space-does-this-model-occupy-then><h3 id=what-space-does-this-model-occupy-then><span class=hanchor arialabel=Anchor># </span>What Space does this Model occupy then?</h3></a><ul><li>This model sits somewhere in scope between the SBF (subsecond to minutes range) and longer scale rhythmic changes like circadian rhythms<ul><li>Ideally it could scale to any time range</li></ul></li><li>The idea is not to create episodic memories of singular stimulus to action mappings</li><li>But create internalized representations of an environmental time landscape, and perhaps map actions to that instead</li><li></li><li>I want to focus on how networks learn from very simple network learning conditions</li><li>I want to be able to create flexible, distributed, and multiplexed informational representations</li><li>No cold storage<ul><li>Is stored in some state of the network and thus its dynamics</li><li>Presumably stays in working memory until phase synchronization with long-term memory forming, slower rhythms is achieved</li></ul></li><li>Non reliant on initial stimulus<ul><li>I don’t have to worry about how the network learns to map a stimulus to a reset trigger</li></ul></li><li>Continual learning, is not “off” or inactive until triggered<ul><li>Instead is constant and informing the</li><li>I want to understand how networks learn based on very simple reward mechanics</li></ul></li></ul><a href=#basic-automata-weighted-vote><h3 id=basic-automata-weighted-vote><span class=hanchor arialabel=Anchor># </span>Basic Automata (Weighted Vote)</h3></a><a href=#adding-receptive-fields><h3 id=adding-receptive-fields><span class=hanchor arialabel=Anchor># </span>Adding Receptive Fields</h3></a><a href=#adding-scaling><h3 id=adding-scaling><span class=hanchor arialabel=Anchor># </span>Adding Scaling</h3></a><p>Supe not done yet</p><a href=#other-aspects><h3 id=other-aspects><span class=hanchor arialabel=Anchor># </span>Other Aspects</h3></a><a href=#using-rpe><h4 id=using-rpe><span class=hanchor arialabel=Anchor># </span>Using RPE</h4></a><a href=#using-decay><h4 id=using-decay><span class=hanchor arialabel=Anchor># </span>Using Decay</h4></a><a href=#measuring-robustness><h4 id=measuring-robustness><span class=hanchor arialabel=Anchor># </span>Measuring Robustness</h4></a><p>The ability to distribute the encoding of multiple time events with negligible loss to accuracy
Test for:</p><ul><li>Simultaneous periodic time events</li><li>Switching RPs<ul><li>Normally covered by scaling tbh</li><li>Dependent on oscillator value distribution</li></ul></li></ul><a href=#oscillator-value-distributions><h4 id=oscillator-value-distributions><span class=hanchor arialabel=Anchor># </span>Oscillator Value Distributions</h4></a><ul><li>Primes</li><li>Uniform</li><li>1-F distribution<ul><li>Uniform form</li><li>Drawn with probability along curve</li></ul></li></ul><a href=#scaling-strategies><h4 id=scaling-strategies><span class=hanchor arialabel=Anchor># </span>Scaling Strategies</h4></a><a href=#implementing-in-snn-with-stdp><h4 id=implementing-in-snn-with-stdp><span class=hanchor arialabel=Anchor># </span>Implementing in SNN with STDP</h4></a><a href=#050723><h1 id=050723><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">05.07.23</a></h1></a><ul><li>Build on <a class="internal-link broken">Outline 2.1</a></li><li>Make a new outline<ul><li>With the purpose of a presentation</li></ul></li></ul><a href=#030723><h1 id=030723><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">03.07.23</a></h1></a><a href=#re-review-of-melloscalablepopulationcode2015mello-et-al-2015><h2 id=re-review-of-melloscalablepopulationcode2015mello-et-al-2015><span class=hanchor arialabel=Anchor># </span>Re-Review of <a class="internal-link broken">Mello et al. (2015)</a></h2></a><ul><li>I need more info on his <strong>decoder</strong></li></ul><a href=#figure-4><h4 id=figure-4><span class=hanchor arialabel=Anchor># </span>Figure 4</h4></a><blockquote class="[!figure 4]-callout"><p>[!Figure 4]
<a class="internal-link broken">SBFA Paper Log-image-20230703140213444.png</a>
Single-Trial Estimates of Elapsed Time Decoded from the Population Response Correlate with True Time during Initial Trials of 12-s and 60-s FI Blocks
<strong>(A)</strong> Decoded population estimates of elapsed time from reward in single trials, for the first seven trials of the 12-s FI block plotted against true time. Red traces indicate the mean of the population likelihood function, and the underlying heatmap indicates the population likelihood function. The last panel shows a seven-trial average likelihood function using the first seven trials of the 12-s block.
<strong>(B)</strong> Decoded estimates of elapsed time for the first seven trials of the 12-s FI block plotted on the same axis. Curves are quadratic fits to the mean likelihood function of each individual trial (red lines in first seven panels). Red curves represent early trials, and black curves represent later trials.
<strong>(C)</strong> Same description as in (A), but for the 60-s FI.
<strong>(D)</strong> Same description as in (B), but for the 60-s FI. See also Figure S4.</p></blockquote><ul><li>So according to A, we should see a relative recovery in 4 trials of 10 FI repetitions?<ul><li>For our purposes that is 40 RPs</li></ul></li></ul><a href=#im-skipping-because-i-cant-focus-rn><h3 id=im-skipping-because-i-cant-focus-rn><span class=hanchor arialabel=Anchor># </span>I’m skipping because I can’t focus rn</h3></a><a href=#tsaoneuralbasestiming2022tsao-et-al-2022><h2 id=tsaoneuralbasestiming2022tsao-et-al-2022><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">Tsao et al. (2022)</a></h2></a><p>Made a glossary page for this <a class="internal-link broken">Tsao et al. (2022) - glossary</a></p><ul><li>Asks if prospective timing is generally related to retrospective timing<ul><li>To which, I would say yes</li><li>I’m actually not sure this is the correct framing they are using here</li><li><del>Like if we are predicting an interval, then it is basically the same</del></li><li>No nvm, it’s the difference between temporal bifuraction task and PIT</li><li>But presumably the same neuron (MSNs) are being activated</li><li>They have an entire box explaining it</li></ul></li></ul><a href=#prospective-timing-and-retrospective-timing-terminology><h4 id=prospective-timing-and-retrospective-timing-terminology><span class=hanchor arialabel=Anchor># </span>‘prospective timing’ and ‘retrospective timing’ terminology</h4></a><blockquote><p>First, retrospective timing is operationally defined as the estimation of a duration when subjects are unaware beforehand that they will estimate the duration, whereas prospective timing is defined as estimation of duration when subjects are aware beforehand161. The logic of this operational definition is that, particularly during prospective timing, subjects have the opportunity to explicitly track the passage of time. Thus, in the case where estimation occurs after the duration has ended but subjects are already aware beforehand that they will eventually estimate duration, this would still be considered prospective timing.</p><p>Second, ‘prospective timing’ and ‘retrospective timing’ refer specifically to the estimation of duration, even though timing encompasses temporal order in addition to duration. Whether there exists a similar distinction between temporal order based on ongoing events as opposed to memory of past events is unclear.</p><p>Finally, ‘prospective timing’ overlaps with other commonly used terms such as ‘interval timing’ and ‘reward timing’, whereas ‘retrospective timing’ partially overlaps with ‘episodic timing’, and each has also been referred to as ‘experienced duration’ and ‘remembered duration’, respectively335.</p></blockquote><a href=#intro><h3 id=intro><span class=hanchor arialabel=Anchor># </span>Intro</h3></a><blockquote><p>Early models proposed that the internal clock would be implemented through a pacemakeraccumulator mechanism11–13 in which pulses generated by a central pacemaker would be integrated by an accumulator to generate estimates of time. However, outside the circadian system14,15, evidence for biological implementation of such a mechanism has been missing.</p></blockquote><ul><li>I don’t get this Gu et al. which is their citation directly refutes this idea</li></ul><a href=#figure-2><h4 id=figure-2><span class=hanchor arialabel=Anchor># </span>Figure 2</h4></a><blockquote class="[!figure 2]-callout"><p>[!Figure 2] Neural trajectories during prospective timing.
<strong>a</strong> | The activity of a population of neurons, or population state, at a given moment can be depicted as a point within a state space where each axis corresponds to the firing rate of a specific neuron.</p><p>In this example, there are three neurons, so the state space is three-dimensional.
As the firing rates of the three neurons change across time, so does the population state, tracing out a trajectory, in this case spanning three time points (T1, T2 and T3).</p><p><strong>b</strong> | For a population consisting of N cells, the state space is N-dimensional. Dimensionality reduction can be used to reduce noise and to project data onto specific subspaces, and often it is helpful to apply dimensionality reduction techniques to more easily identify important features of neural trajectories.</p><p>In this cartoon example, dimensionality reduction reveals a simpler trajectory within a two-dimensional space.</p><p><strong>c</strong> | Different trajectory features are used for sensory timing, motor timing and temporal expectation. For sensory timing of different durations (indicated by different shades of grey), a common trajectory progresses at a constant speed (starting at a common population state and reaching a common intermediate state over the same amount of elapsed physical time, indicated in blue) but stops at different terminal states for specific durations (indicated by different shades of red).</p><p>For motor timing of different durations (indicated by different shades of grey), a common trajectory progresses at different speeds (starting at a common population state and reaching different intermediate population states over the same amount of elapsed physical time, indicated in blue) to reach a common terminal state (different durations indicated by different shades of red).</p><p>For temporal expectation, a trajectory begins to evolve following the start of a fixed temporal structure, and reaches a specific state in accordance with this temporal structure. In the illustrated example, the temporal structure is defined by a cue which occurs at a fixed interval following a start signal, and the trajectory reaches a particular set of states (indicated in purple) at the time of the cue. This expectation-related internal state may then be reflected in an altered response criterion or modulation of sensory representations, for example.</p><p><strong>d</strong> | Left, structure of ‘ready, set, go’ sensorimotor timing task. Monkeys are presented with a sample interval <strong>ts</strong>, which they reproduce as <strong>tp</strong> through the timing of a motor action.</p><p>Top centre, reward (green shading) is a function of the relative error between ts and tp.</p><p>Bottom centre, the set of sample intervals used.</p><p>Right, behaviour from an example monkey illustrating that animals are able to carry out both sensory timing and motor timing accurately in order to reproduce presented intervals. Dots indicate individual trials, and circles indicate average tp per ts.</p><p><strong>e</strong> | Neural trajectories during estimation (left) and production (right) epochs, based on activity recorded from the dorsomedial frontal cortex. These two sets of trajectories illustrate how prospective timing is accomplished through population state dynamics: specific durations are associated either with specific population states along the trajectory (estimation epoch) or specific trajectory speeds (production epoch).</p><p>Triangles indicate ‘ready’, circles indicate ‘set’, squares indicate ‘go’ and small circles indicate neural states at 40-ms increments. Dim, dimension; PC, principal component. Parts d and e adapted with permission from reF.56, Elsevier.</p></blockquote><ul><li>Lmao fuck me they are using PCA dim. reduc. method<ul><li>Which of course could mean</li></ul></li></ul><a href=#writing-ideas><h2 id=writing-ideas><span class=hanchor arialabel=Anchor># </span>Writing ideas</h2></a><ul><li>Oscillatory processes set the foundation for the continuation and homeostasis of an organism<ul><li>Abstractly, this represents the organism as occupying an stable attractor optimization in its environment<ul><li>Which we may call characterize as teh “default state network” for the sake drawing an analogue in oscillations in observed neural data</li></ul></li><li>Cardiac, Pulmonary, Metabolic, Circadian</li><li>As the organism scale increases, task specific modules isolate and take on their own specific rhythms</li><li>That still needs to relate to the rhythms of the whole</li><li>Both contributing to the changing greater loop, and taking instruction from</li></ul></li><li>It follows that rhythmic processes set the basis of many biological functions</li><li>And that rhythmic process related to other rhythmic processes in the organism but some direct relational encoding between respective points in the phase procession of the rhythmic processes in the presence of important stimuli</li><li>Further, it is necessary to the stable organism to remap the relation of phase processions in a changing dynamic environment</li><li>As well as allow for some deviation in accuracy to allow some “exploration” of environmental conditions</li></ul><a href=#dopaminergic-response><h3 id=dopaminergic-response><span class=hanchor arialabel=Anchor># </span>Dopaminergic response</h3></a><p>As reward becomes expected, the anticipatory firing begins tor precede the reward stimulus
Thus perhaps the drift of this response is somehow due to STDP
How would this work?</p><ul><li>The neurons which fire in advance of the correct stimulus are increased in weight</li><li>Thus possibly some drift of the firing neuron to earlier firing neurons increases</li><li>Disinhibition of earlier firing inhibitory neurons (ones that fire after)</li><li>Later firing inhibitory neurons increase?</li><li></li></ul><a href=#270623><h1 id=270623><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">27.06.23</a></h1></a><a href=#finishing-up-on-cohenittime2011cohen-2011-from-yesterday><h2 id=finishing-up-on-cohenittime2011cohen-2011-from-yesterday><span class=hanchor arialabel=Anchor># </span>Finishing up on <a class="internal-link broken">Cohen (2011)</a> from yesterday</h2></a><p>I left off at section <em><strong>How to study time-based processing schemes</strong></em>
And skipped over the next few sections and straight to conclusion</p><blockquote><p>Here it is argued that considerable neural information is embedded in the rich temporal landscape of electrophysiological dynamics, that much of this information may be lost when confining analyses to spatial dimensions, and that at least some of this information can be extracted non-invasively in humans using EEG and MEG. Approaches and analyses focused on temporal dynamical coding schemes will not render useless other approaches that are based on different (e.g., spatial) assumptions of neurocognitive function. However, ideas about time-based information coding schemes, and the approach of examining the temporal dynamics of brain electrical activity, are an important next step in theoretical and empirical human neuroscience developments. This nascent but growing literature on human neural temporal dynamics will provide a new impetus in uncovering fundamental neurocognitive mechanisms, linking research in humans to that in animals, and improving clinical diagnosis and treatment assessment. It’s about time.</p></blockquote><a href=#need-to-describe-method-for-scaling-and-distribution-of-oscillators><h2 id=need-to-describe-method-for-scaling-and-distribution-of-oscillators><span class=hanchor arialabel=Anchor># </span>Need to describe method for scaling and distribution of oscillators</h2></a><p>What if I checked out <a class="internal-link broken">@melloScalablePopulationCode2015</a> again?</p><a href=#260623><h1 id=260623><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">26.06.23</a></h1></a><a href=#picking-back-up-at-section-73><h2 id=picking-back-up-at-section-73><span class=hanchor arialabel=Anchor># </span>Picking back up at section 7.3</h2></a><p><a class="internal-link broken">Declarative Memory</a></p><a href=#models-of-forgetting---section-74><h2 id=models-of-forgetting---section-74><span class=hanchor arialabel=Anchor># </span>Models of forgetting - Section 7.4</h2></a><p>Ok apparently they do have some modeling of holding multiple items in relation to a time period and how they might interfere to cause forgetting</p><blockquote><p>Simultaneous maintenance of different frequencies of theta oscillations (representing multiple items) would be easily disrupted by the interactions from other neuronal groups, especially when the two neuronal groups are tightly connected to each other—a result consistent with interference theory.</p></blockquote><p>Also, <em>multiple EIO neuronal groups</em>.</p><a href=#time-based-resource-sharing-tbrs-model-tp><h3 id=time-based-resource-sharing-tbrs-model-tp><span class=hanchor arialabel=Anchor># </span>time-based resource-sharing (TBRS) model #tp</h3></a><blockquote><p>a time-based resource-sharing (TBRS) model that explains how forgetting is time related (Barrouillet et al., 2004) as well as the manner in which time plays a crucial role in working memory load (Barrouillet et al., 2007).</p></blockquote><blockquote><p>Moreover, the TBRS model of working memory (Barrouillet et al., 2004, 2007) suggests that forgetting is time related such that the proportion of time capturing attention for each item is crucial to memory maintenance. This claim is consistent with the hybrid EIO model to the extent that it predicts the close relationship between interval timing and multiple-item working memory.</p></blockquote><blockquote><p>Similarity-based interference theory proposes that maintained representations compete with other representations held in working memory, which could be similar and/or stronger, and result in interference and forgetting (Nairne, 1990; Oberauer and Kliegl, 2006; Saito and Miyake, 2004). The observation of proactive and retroactive interference (e.g., interference from prior trials in the case of proactive interference or interference from later trials in the case of retroactive interference) on current trials and increased interference with higher similarity support this proposal. Oberauer and Kliegl (2001, 2006) have provided further support for the interference model by fitting the time-accuracy data from a working memory task with a nonlinear mixed-effects model. The basic idea underlying this study was that more similar conditions provoke more interference because interference arises from overlapping features of item representation, and the estimated interference successfully accounted for the time-accuracy data in the working memory paradigm, whereas decay theories could not account for the time-accuracy data. However, as a contradictory point to this argument, Portrat et al. (2008) showed that a time-related decay effect is present as a function of processing time and recall performance in a different working memory paradigm, consistent with the TBRS model proposed by Barrouillet et al. (2004). However, these decay and interference hypotheses, which provide seemingly contradictory accounts for the loss or inability to access stored information, may be supported by the same neuronal properties (Jonides et al., 2008).</p></blockquote><blockquote><p>For example, if the population neurons receive synchronized 6-Hz IO, the 7-Hz theta frequency encoding item 1 would be mainly reactivated at 1.5, 2.5, 3.5 s and so on (entrained within a 1-Hz delta oscillation) while the 8-Hz theta frequency encoding item 2 would be reactivated at 1.25, 1.75, 2.25 s and so on (entrained within a 2-Hz delta oscillation). In this sense, the time should be divided for the reactivations of each item to reduce the temporal overlap of reactivations between multiple items, so the restriction for the multiple-item maintenance originates from the temporal resource sharing (see Buhusi and Meck, 2009a) as proposed by the TBRS model.</p></blockquote><p>So we can say that the weights of our oscillators is the attention and when giving to differing RPs the attention is divided, such that we should see similar interference pattern to the reward “memory” retrieval.</p><a href=#in-absence-of-stimulus><h2 id=in-absence-of-stimulus><span class=hanchor arialabel=Anchor># </span>In absence of stimulus</h2></a><blockquote><p>This hybrid model proposes the shared neural-oscillation properties underlying interval timing and working memory. <strong>Even in the absence of an ongoing stimulus, neural oscillations in the brain continue in time, and this property would explain how we can perceive time and maintain information in the brain</strong>. Certain frequency ranges of oscillation would include the specific dimension of information; such as theta activity entrained in delta for duration information and gamma activity entrained in theta for other stimulus attributes (e.g., pitch). In this way, interval timing and working memory would originate from the same cortical representations, but the specific connections between striatal and cortical neurons would be able to extract the different dimensions of that information.</p></blockquote><p>So can we say this is a model of ongoing time oscillations?</p><a href=#closing-remarks-on-importance-of-oscillatory-study><h2 id=closing-remarks-on-importance-of-oscillatory-study><span class=hanchor arialabel=Anchor># </span>Closing remarks on importance of oscillatory study</h2></a><blockquote><p>recent focus on the temporal/oscillatory properties of brain and behavior has opened up another dimension for understanding the cognitive architecture of the brain (e.g., Cohen, 2011). Interval timing and working memory in particular share the characteristic that some internal process must continue over the course of time regardless of the existence of an external stimulus.</p></blockquote><blockquote><p>spatio-temporal patterns of activation supporting these cognitive functions are sustained in neural networks over a time and these neural networks can be managed through the oscillatory fluctuations that reside in the recurrent networks. Because of the importance of these neural dynamics, the temporal/oscillatory properties of the brain should be emphasized in future studies of interval timing, attention, and working memory (see, for example, Cohen, 2011;Henry and Herrmann, 2014; Lake et al., 2014; Rohenkohl and Nobre, 2011; Rohenkohl et al., 2012)</p></blockquote><blockquote><p>it has been hypothesized that the slower-frequency oscillations are involved in long-range communication in the brain (e.g., Buzsáki, 2006; Buzsáki and Draguhn, 2004; Buzsáki et al., 2012; Murray et al., 2014).</p></blockquote><ul><li>Lower frequencies correlate to “higher” or larger scale brain comms</li></ul><a href=#picked-up-two-references-i-need-to-check><h2 id=picked-up-two-references-i-need-to-check><span class=hanchor arialabel=Anchor># </span>Picked up two references I need to check</h2></a><p><a class="internal-link broken">@collinsHowMuchReinforcement2012</a>
<a class="internal-link broken">@cohenItTime2011</a></p><a href=#cohenittime2011><h2 id=cohenittime2011><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">@cohenItTime2011</a></h2></a><blockquote><p>Here I will argue that too much attention has been focused on investigating neurocognitive function based on attempts to localize processes in space (i.e., functional localization). Instead, fruitful insights might arise from considering time to be an important factor in neurocognitive function. Indeed, for some neurocognitive processes, time may be as important, or possibly more important, than space in terms of the underlying neurocomputational mechanisms.</p></blockquote><ul><li>Good citation for how time has not been a matter of focus</li></ul><blockquote><p>It is likely that the brain uses more dimensions for information processing than just space and activation magnitude. This is not meant to imply that space is irrelevant for information coding/processing, or that functional localization is inappropriate or invalid. Rather, after this initial period of studying functional localization and learning about its merits and limitations, it is perhaps useful to consider time as an important factor</p></blockquote><blockquote><p>time may be as important – if not more important – than space for information processing, particularly at the level of small populations of cells (spatial scale of millimeters to a few centimeters). As described below, “time” refers to rapid dynamics in electrochemical signals that are often but not necessarily oscillatory</p></blockquote><blockquote><p>Time as latency in functional MRI (e.g., the hemodynamic response peaks about 6–8 s after a stimulus) or an event-related component (e.g., the average voltage deflection 300–600 ms after a stimulus) is not taking into account the rich information that appears to be embedded in the temporal dynamics of neural activity.</p></blockquote><a href=#several-empirical-and-theoretical-reasons-why-time-may-be-in-important-factor-in-neural-information-processing><h3 id=several-empirical-and-theoretical-reasons-why-time-may-be-in-important-factor-in-neural-information-processing><span class=hanchor arialabel=Anchor># </span>several empirical and theoretical reasons why time may be in important factor in neural information processing</h3></a><blockquote><p><strong>(1)</strong> There appears to be information carried in the precise timing of activity within and across physically separated areas of the brain that cannot be measured by overall activity levels in any individual brain region. “Information” here can refer simply to quantifiable measures of brain activity that predict the cognitive state or behavioral response of the subject. In some cases, temporal dynamics of neural activity are significantly related to the task events while the overall amount of activity averaged over time is not. These kinds of results provide direct evidence that information in the brain is embedded in the rich temporal landscape of electrophysiological activity, and is lost when averaging activity over larger periods of time. Examples will be outlined in a subsequent section.</p></blockquote><blockquote><p><strong>(2)</strong> In an information-theoretic sense, time provides a large number of possibilities for information to be represented and processed continuously, rapidly, and simultaneously (in parallel) in multiple functionally distinct networks that overlap in time and space. Time provides a rich source of complex multi-dimensional data in which information can be represented and processed. <strong>The large amount of information provided by the temporal dynamics of neural activity arises in part because electrophysiological activity of the brain is strongly oscillatory</strong>. These oscillations reflect rhythmic fluctuations in the excitability of populations of neurons (Tiesinga et al., 2008; Wang, 2010).</p></blockquote><blockquote><p>Oscillations occur in multiple temporal and spatial scales, ranging from ultra-slow oscillations with a periodicity of tens of seconds over much of the cortex during deep sleep (Steriade, 2006) to ultra-fast oscillations with a periodicity of a few milliseconds within patches of somatosensory cortex (Curio, 2000). Oscillations that seem most relevant for cognitive processes range from delta (∼1–4 Hz) and theta (∼4–8 Hz) to gamma (∼30–100 Hz; for general reviews of neural oscillations, see Varela et al., 2001; Buzsaki and Draguhn, 2004; Traub et al., 2004).</p></blockquote><blockquote><p>Because activity in one frequency band may occur independently of, and in parallel with, activity in other frequency bands, there is considerable bandwidth for information processing. For example, it has been suggested that multiple alpha sub-bands can be functionally dissociated in their roles in memory processes (Klimesch et al., 2007). Thus, if different neurons are “tuned” to different frequency bands (Jacobs et al., 2007), multiple functionally distinct neural networks can spatially coexist and be dissociated according to frequency band or spatiotemporal patterns (Akam and Kullmann, 2010).</p></blockquote><blockquote><p>from the activity recorded from a single electrode, there are already multiple domains of information, including
<strong>frequency (the speed of the oscillation</strong>),
<strong>power (the amount of the energy in a frequency band at a point in time</strong>), and
<strong>phase angle (the position of the oscillation along the sine wave, driven by the state of excitation of the population of neurons</strong>; Figure 2; see also Makeig et al., 2004).
And because these dimensions are largely independent of each other, a single electrode in a single location in the brain can measure multi-dimensional local neural dynamics.</p></blockquote><blockquote><p><strong>(3)</strong> There is arguably selection pressure for individuals and species carrying neural systems that can decode the sensory world, make decisions, and adapt behavior faster and more efficiently.</p></blockquote><ul><li>The rest of this section didn’t add anythin especially interesting other than discussing natural pressure for quick response times</li></ul><blockquote><p><strong>(4)</strong> Neural activity is inextricably linked to cognition and behavior in time, but not in space.</p></blockquote><blockquote><p>The fact that brain activity is time-locked, rather than space-locked, to behavior implies that time will be highly informative about behaviorally relevant neurocognitive mechanisms. The brain does indeed exhibit some spatial relationships with the body (e.g., homuncular organization of sensorimotor regions, retinotopic organization of visual areas), although these examples still exhibit some arbitrary relations to behavior, such as the left–right and up–down crossovers.</p></blockquote><blockquote><p><strong>(5)</strong> Controversies develop in cognitive neuroscience over the precise functional role of a specific region, but some of these controversies may be moot because multiple functionally distinct neural networks may coexist in the same space. Indeed, in these cases, empirical evidence may seem conflicting because different theories can be supported by different experiments.</p></blockquote><blockquote><p>it seems likely that functionally different networks can emerge from the same population of anterior cingulate cortex neurons, depending on task demands (Fujisawa et al., 2008).</p></blockquote><blockquote><p>In other words, rather than attempting to resolve a grandunified-theory for the function of a region of the brain (in this example, the anterior cingulate cortex), attention might be better spent trying to understand how that area may utilize temporal schemes to compute and coordinate the diverse functions suggested by empirical evidence.</p></blockquote><ul><li>This is better emphasized by the fact that neuronal circuits are highly recurrent and deeply entangled, meaning that computation and information is being encoded in high dimensional dynamics of the activity which shares multiple functional attributes simultaneously.</li></ul><a href=#examples-of-time-embedded-information-in-human-electrophysiological-activity><h3 id=examples-of-time-embedded-information-in-human-electrophysiological-activity><span class=hanchor arialabel=Anchor># </span>examples oF time-embedded inFormation in human electrophysiological activity</h3></a><p>I was looking for something on coding embeddings</p><blockquote><p>Although the literature on time-based coding schemes and sophisticated analyses of electrophysiological data is overshadowed by the literature on fMRI-based localization studies, <strong>there are too many relevant and insightful findings to discuss all them all here. Instead, this section will highlight three examples</strong> of how mathematical analyses of the temporal dynamics of human electrophysiological recordings have shed insight into neurocognitive function. These examples also illustrate cases in which standard localization- and hemodynamic-based analyses would be unlikely to reveal these brain dynamics (e.g., because no overall increase in space-averaged activity occurs).</p></blockquote><a href=#cross-frequency-coupling-tp><h4 id=cross-frequency-coupling-tp><span class=hanchor arialabel=Anchor># </span>Cross-frequency coupling #tp</h4></a><blockquote><p><strong>(1)</strong> Cross-frequency coupling refers to a relationship between activities in two different frequency bands. For example, the power of gamma (∼30–80 Hz) oscillations may vary as a function of the phase of theta (∼4–8 Hz). Cross-frequency coupling may be used for information coding if the lower frequency oscillations coordinate the activity of sub-populations of cells that use higher frequency oscillations to process information.</p></blockquote><blockquote><p>There are several ways in which cross-frequency coupling can be quantified (Mormann et al., 2005; Canolty et al., 2006; Cohen, 2008; Tort et al., 2010); different methods may be suited for different purposes, but all methods generally test for a modulation of activity in one frequency band as a function of activity in another (typically, relatively lower) frequency band.</p></blockquote><blockquote><p>There are several ways in which cross-frequency coupling can be quantified (Mormann et al., 2005; Canolty et al., 2006; Cohen, 2008; Tort et al., 2010); different methods may be suited for different purposes, but all methods generally test for a modulation of activity in one frequency band as a function of activity in another (typically, relatively lower) frequency band.</p></blockquote><ul><li><a class="internal-link broken">Lisman and Jensen (2013)</a><ul><li>References another paper from Lisman but I think it’s basically the same idea</li><li>Directly related to the EIO-SBF as it entrains with this exact rhythms</li></ul></li></ul><a href=#inter-regional-oscillatory-synchronization-tp><h4 id=inter-regional-oscillatory-synchronization-tp><span class=hanchor arialabel=Anchor># </span>Inter-regional oscillatory synchronization #tp</h4></a><blockquote><p><strong>(2)</strong> In addition to dynamics across frequency bands within the same region of space, information may be embedded in the temporal relationship of activity over space. Inter-regional phase synchronization (a frequency band-specific measure of functional connectivity) may underlie information transfer and co-processing (Knight, 2007; Womelsdorf et al., 2007).</p></blockquote><ul><li>Also mentioning <em>long range synchronization</em></li></ul><blockquote><p>And because changes in phase synchronization may occur without any concomitant changes in power (Heinzle et al., 2007), there might be information embedded in the temporal relationship between areas that is not localized to either region alone.</p><p>For example, inter-regional oscillatory synchronization may be the mechanism by which the medial frontal cortex interacts with other brain systems, such as lateral prefrontal cortex to implement cognitive control after errors in speeded reaction-time (Hanslmayr et al., 2008; Cavanagh et al., 2009) or reinforcement learning (Cavanagh et al., 2010) tasks, with occipital cortex to bias sensory processing during go/no-go tasks in which no-go cues were difficult to perceive (Cohen et al., 2009d), or with the nucleus accumbens during reinforcement learning and reward anticipation (Cohen et al., 2009b, 2011).</p></blockquote><a href=#microstates-and-other-transient-electrophysiological-events-tp><h4 id=microstates-and-other-transient-electrophysiological-events-tp><span class=hanchor arialabel=Anchor># </span>Microstates and other transient electrophysiological events #tp</h4></a><blockquote><p><strong>(3)</strong> Microstates refer to brief periods of cortical electrophysiological activity that are topographically stable over tens to hundreds of milliseconds (Lehmann et al., 2006). Microstates fluctuate 1–2 orders of magnitude faster than the hemodynamic response, and have been linked to visual perception, error processing, and resting state (Muller et al., 2005; Britz and Michel, 2010).</p></blockquote><blockquote><p>They are sometimes accompanied by hemo-dynamic responses (Britz et al., 2010; Musso et al., 2010). Other brief cortical events include endogenous “bursts” of frontal alpha asymmetry (Allen and Cohen, 2010) that have been linked to depression. Transient bursts of synchronized electrophysiological activity also occur during sleep, namely spindles and ripples, which have been linked to memory formation and dream recall (Axmacher et al., 2008).</p></blockquote><a href=#collinshowmuchreinforcement2012><h2 id=collinshowmuchreinforcement2012><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">@collinsHowMuchReinforcement2012</a></h2></a><ul><li>RL</li><li>Working memory</li></ul><p>They use a softmax func to choose an action:</p><blockquote><p>where $a_{\mathrm{RL}}$ is the learning rate. Choices are generated probabilistically as a function of the difference in $Q$ values between the available actions using the softmax choice rule:
$$
p(a \mid s)=\frac{\exp \left(\beta_{R L} Q(s, a)\right)}{\sum_i \exp \left(\beta_{R L} Q\left(s, a_i\right)\right)}
$$
where $\beta_{\mathrm{RL}}$ is an inverse temperature determining the degree to which differences in $Q$ values are translated into a more deterministic choice.</p></blockquote><ul><li>Perhaps I could do something similar.</li><li>What exactly does this do?</li></ul><blockquote><p><strong>The number of stimuli, denoted as set size $n_S$,</strong></p></blockquote><a href=#forgetful-reinforcement-learning-rlf-model-tp><h3 id=forgetful-reinforcement-learning-rlf-model-tp><span class=hanchor arialabel=Anchor># </span>Forgetful reinforcement learning (RLF) model #tp</h3></a><blockquote><p>In this model, $Q$ learning and action selection occur as in Eqns 1 and 2 from model RL2. Additionally, we include a supplementary effect of forgetting across time; at each trial, for all stimulus-action pairs ( $S$, a), we decay $Q$ values towards their initial values
$$
Q(s, a) \leftarrow Q(s, a)+\epsilon \times\left(Q_0-Q(s, a)\right)
$$
where $Q_D=1 / n_A$ is the initial $Q$ value for all actions, representing random policy, and $\epsilon$ controls the degree of forgetfulness.</p><p>Thus, with increasing delay between repeated encounters with the same stimulus, the more the learned $Q$ values will have decayed. Consequently, with $\in>0$, this three-parameter model predicts a decrease in performance in higher set-size blocks, due solely to the increased average delay between stimulus repetitions for higher set sizes (see Fig. 4).</p></blockquote><a href=#figure-4-1><h4 id=figure-4-1><span class=hanchor arialabel=Anchor># </span>Figure 4.</h4></a><p><a class="internal-link broken">image-20230626190944890.png</a></p><blockquote><p><strong>Fig. 4</strong>. Model results. Learning curves as a function of set size for each model, generated using best-fit parameters for each subject given the model. Models: RL2, two-parameter RL model; WM, pure WM model; RL6, RL model with five learning rates + one softmax temperature; RLF, three-parameter forgetful model; RL + WM, mixture RL and WM model. Subjects, observed learning curves across all subjects (from Fig. 2).</p></blockquote><a href=#reinforcement-learning--working-memory-model-mathrmrlmathrmwm-tp><h3 id=reinforcement-learning--working-memory-model-mathrmrlmathrmwm-tp><span class=hanchor arialabel=Anchor># </span>Reinforcement learning + working memory model ($\mathrm{RL}+\mathrm{WM}$) #tp</h3></a><p>In the $\mathrm{RL}+\mathrm{WM}$ model, action selection derives from a mixture of a pure simple RL model (i.e. RL2 ) [By RL2 they meal an RL with two parameters (s, a)] applied to all set sizes [number of stimuli], together with a limited capacity WM component.</p><p>We simulated WM as the encoding of an observed event that, if maintained in memory, could serve to immediately and robustly affect behavior.[ WHAT THE FUCK DOES THIS MEAN] That is, perfect memory could be represented by a $Q$ learning system with a learning rate of 1 (which is optimal for a deterministic task). However, memory degrades over time and is capacity-limited. For the degradation effect, we implement a decay as in the RLF model so that, after RL update, for all $(s, a)$ at each trial
$$
Q_{\mathrm{WM}}(s, a) \leftarrow Q_{\mathrm{WM}}(s, a)+\epsilon \times\left(\frac{1}{n_{\mathrm{A}}}-Q_{\mathrm{WM}} Q(s, a)\right)
$$
The probability of action selection according to the $\mathrm{WM}$ component is then $p_{\mathrm{WM}}(a)=$ $\operatorname{softmax}\left(\beta_{\mathrm{WM}} Q_{\mathrm{WM}}\right)$. As in earlier models, the probability of action selection for the RL component is $p_{\mathrm{RL}}(a)=\operatorname{softmax}\left(\beta_{\mathrm{RL}} Q_{\mathrm{RL}}\right)$. As stated thus far, the WM component captures forgetting but does not yet account for the known limited capacity of WM. This capacity limitation is factored into the mixture weight $W(t)$ determining the probability that action selection is governed by the RL or WM component
$$
p(a)=(1-w(t)) p_{\mathrm{RL}}(a)+w(t) p_{\mathrm{WM}}(a)
$$</p><a href=#220623><h1 id=220623><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">22.06.23</a></h1></a><a href=#left-off-at-sec-63-in-guoscillatorymultiplexingneural2015a><h2 id=left-off-at-sec-63-in-guoscillatorymultiplexingneural2015a><span class=hanchor arialabel=Anchor># </span>Left off at Sec. 6.3 in <a class="internal-link broken">@guOscillatoryMultiplexingNeural2015a</a></h2></a><p>This is really long and there are only 2 hours left in the day. Time to go home.</p><p>See how the gamma and theta cycles are described here:</p><blockquote><p>In relation to the importance of theta and gamma oscillations for working memory, Lisman and <a class="internal-link broken">Declarative Memory</a>colleagues (e.g., Lisman, 2005, 2010; Lisman and Idiart, 1995) proposed an oscillatory model of working memory. According to this model, gamma oscillations entrained within theta could represent maintained memory by repetitively activating relevant neuronal groups with temporal precision, and multiple items can be maintained in working memory by the multiple cycles of sequential gamma oscillations entrained within a theta oscillation (Jensen, 2006; Jensen and Lisman, 1998; Lisman, 2010; Lisman and Idiart, 1995—Fig. 3).
<a class="internal-link broken">image-20230622133319494.png</a></p></blockquote><blockquote><p>For example, different items in memory are represented by different neuronal groups (e.g., spatial pattern of cells), and each neuronal group is activated during each gamma cycle. Because multiple <strong>gamma cycles of 30–80 Hz</strong> are present within each <strong>theta cycle of 4–10 Hz</strong>, multiple item information/memory represented by multiple gamma cycles is activated every theta cycle so that they can be maintained as separated from each other within a specific temporal sequence.</p></blockquote><p>So basically sequences are mapped to the larger theta wave, and the individual items of the sequence are mapped to the smaller gamma waves</p><p>Phase precession of cells wrt to theta waves. I.e. you can measure distance traveled via cell firing in location in theta phase:</p><blockquote><p>the firing of place cells shows theta-phase precession during spatial navigation (Fig. 4A). For example, as a subject reaches its target area, the place-cell firing occurs at earlier phases of the theta oscillation of the LFP so that the firing phase of the cell reflects the relative distance that the subject has traveled through the cell’s place field (e.g., firing at late phases of theta indicates that the subject has just entered the place field—Burgess et al., 1994; Skaggs et al., 1996).</p></blockquote><p>See <a class="internal-link broken">Figure 5 of Gu et al. and their description of the EIO model</a></p><ul><li>I have several comments on figure 5</li><li>Like, why, to all of it</li><li>They do EIO at multiple scales like four times and keep introducing new rythms</li><li>Sorta starting to make sense<ul><li>If I am understanding it correctly each of the larger delta and theta waves are emergent descriptors of the underlying gamma waves</li><li>Or not. I think they are saying the Theta IO wave in 5C is from a larger population</li></ul></li><li><strong>They simulate a distribution of 1000 neurons with gamma EO freqs with mean 47Hz and SD 2Hz</strong></li></ul><p>There is a section, sec 7.1, which tries to resolve the issue where individual neurons fire more frequently than the slower population frequency. The resolution they reach is that the Inhib cells are locked to the pop.freq. while excit. neurons have a diverse spread of activity.</p><ul><li>This is the main thesis but not certain how they arrived at it / what it means</li><li></li></ul><blockquote><p>For example, before any triggering event, both excitatory and inhibitory inputs oscillate at a 6-Hz frequency, cancelling each other out. The onset of a to-be-timed signal and the resulting DA input to the cortical neurons (see Allman and Meck, 2012; Matell and Meck, 2004) triggers a sudden increase of EO frequencies in each neuron while leaving the IO synchronized at a slightly lower frequency than the EO. Depending on the density of synaptic connections or by any other reason of cellular diversity, the excitatory inputs to each individual neuron will oscillate at different frequencies, for example, Neuron 1, 2, 3, and 4 will receive 6.2 Hz, 6.35 Hz, 6.5 Hz and 6.65 Hz of EO input, respectively as illustrated in <strong>Fig. 6A</strong>.</p></blockquote><blockquote><p>The summation of EO and IO inputs in each cortical neuron will produce a membrane potential oscillation (MPO) of theta oscillations, enveloped in delta oscillations as previously described. The different frequencies of EO will produce different frequencies of theta MPO, entrained in different delta oscillations. For example, if IO inputs are synchronized at 6 Hz, the MPO of Neuron 1 will show a 6.1-Hz theta oscillation entrained within a 0.2-Hz delta oscillation, whereas Neuron 3 will show a 6.25-Hz theta entrained within a 0.5-Hz delta oscillation as illustrated in <strong>Fig. 6B</strong>.</p></blockquote><blockquote><p>After applying a membrane potential threshold for generating spikes, each neuron will show a different pattern of firing rates across time. For example, Neuron 1, whose EO and IO summation produces a theta oscillation entrained within a 0.2-Hz delta oscillation, would show high firing rates of spikes with 0.2 Hz (every 5 s—so at 2.5 s, 7.5 s, 12.5 s, and so on). Similarly, Neuron 3 with a 0.5-Hz delta oscillation would show high firing rates of spikes with 0.5 Hz (every 2 s—so at 1 s, 3 s, 5 s, and so on).</p></blockquote><blockquote><p>Under these conditions MSNs in the striatum could be trained to detect the coincident firing of a subset of cortical neurons to encode/detect a specific target duration, as well as strengthening the synaptic weights among the specific MSNs and cortical neurons (as originally proposed by the SBF model). If both of the cortical neurons in our previous example (e.g., Neurons 1 and 3) are strongly connected to a certain group of MSNs, then those MSNs will receive a high firing rate of spikes at approximately 3 s as illustrated in <strong>Fig. 6E</strong>.</p></blockquote><blockquote><p>On the other hand, if a different group of MSNs is strongly connected to other cortical neurons whose firing rates peak at 4 s (e.g., Neurons 2 and 4 as illustrated in Fig. 6B), then this group of MSNs will play the role of 4-s detector. In this way, each MSN (or group of MSNs) can detect the coincident firing of multiple cortical subunits, and different MSNs can encode different durations by being connected to a different subset of cortical neurons as illustrated in <strong>Fig. 6E</strong>.</p></blockquote><p>I kinda don’t like how this just “encodes” time by the MSNs just happening to be connected to one or more “oscillator” effectively.
It so happens in this example they give two “oscillators” which inform the MSN. Perhaps this would involve multiple?</p><ul><li>It does say “by being connected to a particular subset”</li><li>So when those are firing, and a MSN is rewarded, thus pushing it over the firing boundary it STDPs to the subset of EIO which was firing with it</li><li></li></ul><blockquote class=figure-callout><p>Figure 6
<a class="internal-link broken">image-20230622152648112.png</a>
<strong>(A)</strong> Theta range of EO (blue) and IO (red) inputs to each cortical neuron are balanced before a triggering event (e.g., dopamine input). The timing onset drives the EO inputs to individual cortical neurons oscillate in the faster and various frequencies while IO inputs are synchronized such as in 6 Hz.</p><p><strong>(B)</strong> Summation of EO and IO inputs in each neuron generates MPO in theta entrained in delta oscillation. The envelope delta frequency differs by the theta frequency of each individual neuron.</p><p>For example, larger theta frequency of Neuron 4 produces the faster delta frequencies. Neurons 1 and 3 (Orange) exhibit a peak around 3 s, but not at 4 s; Neurons 2 and 4 (Cyan) exhibit a peak at 4 s, but not at 3 s. Detection of coincident firing of the relevant neuronal groups such as Neurons 2 and 4 will produce the timing of the 4-s target duration.</p><p><strong>(C)</strong> <strong>Simulation of theta EO frequencies of 1000 cortical neurons is modeled with a mean of 6.5 Hz and SD of 0.2 Hz while IO frequency is fixed at 6 Hz</strong>.</p><p><strong>(D)</strong> <strong>Simulation of total spike inputs from the cortex to the striatum. It shows relatively little spiking between 0 and 0.5 s after the DA input and shows a peak at 0.5–1 s</strong>.</p><p><strong>(E)</strong> Spikes to each MSN neuron from the cortex show a peak at the target time of each MSN, but also exhibit fluctuating patterns across time. For example, 3 s MSN receives peak inputs at 3 s, however, the inputs fluctuate and oscillate in time. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</p></blockquote><a href=#cell-mechanisms-and-simulation-parameters><h4 id=cell-mechanisms-and-simulation-parameters><span class=hanchor arialabel=Anchor># </span>Cell Mechanisms and Simulation Parameters</h4></a><blockquote><p>This new EIO model assumes that the frequency of EO varies across neurons, but this varying frequency across neurons is relatively constant given a particular stimulus.</p><p>The underlying mechanisms of various EO frequencies (and the synchronized IO) should receive further consideration; however, different amounts of recurrent excitatory inputs to each cell or different effects of DA at each neuron or different conductance ratios of neurons could be major factors contributing to the variation in oscillation frequencies (e.g., Gasparini and Magee, 2006; Magee, 2001).</p></blockquote><blockquote><p>In our simulated model the distribution of EO frequencies is defined as a normal distribution with a mean of 6.5 Hz and a standard deviation of 0.2 Hz as illustrated in <strong>Fig. 6C</strong>. The IO frequency was fixed at 6 Hz for a particular neuronal population, based upon evidence from the previous reports (Geisler et al., 2010; Matell and Meck, 2004).</p></blockquote><p>Ok so there was 1000 EO neurons each with a particular freq. which was given by a gaussian distribution around 6.5Hz. See Fig.6c.</p><p>Does this mean there were effective 1-1 IO neurons for each EO?</p><a href=#variation--scaling><h4 id=variation--scaling><span class=hanchor arialabel=Anchor># </span>Variation / scaling</h4></a><p>It looks like they propose changing the variance of an “oscillator” by speeding up the firing rate of an individual EO neuron.</p><blockquote><p>Variation in the EO frequencies will play an important role in encoding various target durations and also a change in the frequency distribution can cause some important behavioral phenomena. For example, if the distribution of EO frequencies is shifted rightwards (increased) without changes in IO, the same firing peak will be reached sooner than the physical time, so that the learned target duration will be reproduced shorter than it should be. As proposed in the SBF model, DA input to the cortex would be able to determine the clock speed by modulating the EO frequencies. However, compared to the SBF model where the whole network should speed up or slow down to same degree, this new model allows for more variation in the oscillation speed of each neuron. Each neuron can speed up by a slightly different amount independently (e.g., the EO frequency of Neuron 2 can change from 6.35 Hz to 6.36 Hz, while the EO frequency of Neuron 4 can remain constant at 6.65 Hz). This would result in an increase in the variability of the behavioral output without seriously disturbing the accuracy of the timing.</p></blockquote><a href=#no-cold-storage><h4 id=no-cold-storage><span class=hanchor arialabel=Anchor># </span>No cold storage</h4></a><blockquote><p>the EIO account does not require a separate accumulator for temporal information, nor does it require a specialized pacemaker providing the temporal information, but it instead derives temporal information from the dynamics of other cognitive components such as the encoding and storage of new information in memory</p></blockquote><a href=#leaving-off-at-section-73-ish><h2 id=leaving-off-at-section-73-ish><span class=hanchor arialabel=Anchor># </span>Leaving off at section 7.3-ish</h2></a><a href=#210623><h1 id=210623><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">21.06.23</a></h1></a><p>Following yesterday</p><a href=#i-think-what-im-getting-from-all-this-is><h2 id=i-think-what-im-getting-from-all-this-is><span class=hanchor arialabel=Anchor># </span>I think what I’m getting from all this is:</h2></a><p>That my experiment, which has no initial stimulus and thus no initial stimulus</p><p>Is instead testing for a midscale time interval, where unstimulated reward is to be timed, and is reliant on longer free-associating oscillators, perhaps made of the smaller SBF oscillator collections</p><p>In that we have multiple oscillator circuits that are just freely running as indicative of a larger stable state, and stimulus is mapped directly to their “phase-stamp”</p><p>How would our network state change to represent changes in intervals, as well as multiple intervals?
Is it possible to entangle the information multiple intervals in a smaller network which does not rely on on single dedicated coincidence detectors for each interval?</p><p>Or perhaps populations of MSNs are what react and change (i.e. our chances of acting via weights). I.e. if multiple MSNs are tuned to our “action” instead of “multiple actions each with some MSN”, then the percentage of the population that is tuned to our oscillators, is the “weight” I.e. more MSNs will tune to the “correct” oscillators and will be inhibited to the “wrong ones”.</p><p>Or something like that. It may be possible .</p><a href=#some-writing-blurbs><h2 id=some-writing-blurbs><span class=hanchor arialabel=Anchor># </span>Some writing blurbs</h2></a><p>lol forgot what it was.</p><p>It was a more general introduction: “The SBF is a well supported model…etc….”</p><p>Oh yeah start writing on how it is more in line with a state-dependent network, and distributes its encodings.</p><p>I think I am more interested in how oscillation in the brain are related to one another than strictly just finding “coincidence”. How is a continuous time-mapping created in the brain. Thus, this is sort of moving towards a variant of the SBF.</p><p>The Model does rely on a continuous and plastic mapping, however we are attempting it in a discrete framework</p><a href=#tekipersistencememoryhow2017teki-et-al-2017><h2 id=tekipersistencememoryhow2017teki-et-al-2017><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">Teki et al. (2017)</a></h2></a><blockquote><p><strong>Entorhinal ‘grid cells’ are also probably a source of temporal information for these ‘time cells’ [
<a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6004118/#R65 rel=noopener>65</a>]</strong> .</p></blockquote><a href=#check-buzsakibrainrhythmshave2023buzsáki-and-vöröslakos-2023-fo-information-on-the-delta-and-theta-rhythms><h3 id=check-buzsakibrainrhythmshave2023buzsáki-and-vöröslakos-2023-fo-information-on-the-delta-and-theta-rhythms><span class=hanchor arialabel=Anchor># </span>Check <a class="internal-link broken">Buzsáki and Vöröslakos (2023)</a> fo information on the Delta and Theta rhythms</h3></a><blockquote><p>Specifically, <em>the model suggests that duration information can be extracted from theta oscillations entrained in the <strong>delta rhythm</strong> while item information in working memory can be extracted from gamma oscillations entrained in the <strong>theta rhythm</strong> respectively</em>. Moreover, this proposal is supported by recent electrophysiological studies [45–47].</p></blockquote><a href=#citations-used-for-original-sbf-model><h3 id=citations-used-for-original-sbf-model><span class=hanchor arialabel=Anchor># </span>Citations used for original SBF model</h3></a><blockquote><ol start=41><li>Allman MJ, Meck WH. Pathophysiological distortions in time perception and timed performance. Brain. 2012; 135:656–677. [PubMed: 21921020] [• Examines the striatal beat-frequency (SBF) model of interval timing from the point of view of different neurological and psychological conditions (e.g., Parkinson’s disease, schizophrenia, attention deficit hyperactivity disorder, and autism). The review also describes the basics of the SBF model and provides full simulation parameters in the appendix.]</li></ol></blockquote><blockquote><ol start=42><li>Lustig C, Matell MS, Meck WH. Not “just” a coincidence: Frontal-striatal synchronization in working memory and interval timing. Memory. 2005; 13:441–448. [PubMed: 15952263]</li></ol></blockquote><blockquote><ol start=43><li>Matell MS, Meck WH. Cortico-striatal circuits and interval timing: coincidence detection of oscillatory processes. Cogn Brain Res. 2004; 21(2):139–170.</li></ol><ul><li><a class="internal-link broken">Teki et al. (2017)</a></li></ul></blockquote><a href=#being-taken-to-lismanthetagammaneuralcode2013-for-information-about-working-memory><h3 id=being-taken-to-lismanthetagammaneuralcode2013-for-information-about-working-memory><span class=hanchor arialabel=Anchor># </span>Being taken to <a class="internal-link broken">@lismanThetaGammaNeuralCode2013</a> for information about “working memory”</h3></a><p><a class="internal-link broken">Working Memory</a></p><a href=#excitatory---inhibitory-oscillation-eio-sbfeio-sbf><h3 id=excitatory---inhibitory-oscillation-eio-sbfeio-sbf><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">EIO-SBF</a></h3></a><p>Breaking it down, we have two oscillators of importance:</p><ul><li><strong>the Excitatory Oscillator (EO</strong>):<ul><li>Per time cell oscillation period</li><li>Resets with stimuli, I think</li></ul></li><li><strong>the Inhibitory Oscillator (IO)</strong>:<ul><li>The synchronized to a global (or local) rythm</li><li>So would be the steady lifetime oscillator</li></ul></li><li>their resultant interference pattern is what causes the “peaks”<blockquote><p>EO and IO will oscillate in beta rhythms and the summation of EO and IO inputs in each neuron will produce an interference pattern of beta entrained in delta or a slower rhythm</p></blockquote></li><li>Why is this required instead of some population of synchronous activity for each time cell oscillator?</li></ul><p><strong>Are Beta Rhythms oscillations in the frequency range of 15-35Hz?</strong></p><a href=#integration-of-hpc-time-cells><h3 id=integration-of-hpc-time-cells><span class=hanchor arialabel=Anchor># </span>Integration of HPC time cells</h3></a><blockquote><p>The capacity to support relational memory makes the hippocampus an ideal brain structure for temporal integration [56, 57], as temporal connections are implicitly made between events.</p></blockquote><a href=#time-cells-first-discovered-in-reference><h4 id=time-cells-first-discovered-in-reference><span class=hanchor arialabel=Anchor># </span>TIME CELLS first discovered in reference:</h4></a><ol start=58><li>MacDonald CJ, Carrow S, Place R, Eichenbaum H. Distinct hippocampal time cell sequences represent odor memories in immobilized rats. J Neurosci. 2013; 33(36):14607–14616. [PubMed: 24005311]</li></ol><blockquote><p>The temporal reference frame in hippocampal ‘time cells’ was first identified in the CA1 pyramidal layer of rats by MacDonald and colleagues [58]. Their findings provided support for the proposal that hippocampal ‘time cells’ signal both temporal and spatial information on a continuum.</p><p>This was based on the observation that when the duration of a delay period was suddenly changed, largely new sequential patterns of activity emerged. Thus, just as ‘place cells’ remap to represent different spatial contexts, these ‘time cells’ adjusted (what the authors referred to as “retimed”) in order to represent a different temporal context.</p><p>Consequently, these hippocampal neurons are referred to as ‘time cells’ because they share many of the same general properties of ‘place cells’, but are instead correlated with the temporal domain.</p></blockquote><a href=#more-about-grid-cells><h3 id=more-about-grid-cells><span class=hanchor arialabel=Anchor># </span>More about Grid Cells</h3></a><p><a class="internal-link broken">@tekiPersistenceMemoryHow2017</a></p><a href=#fuck-i-dont-see-any-actual-simulation-methods-in-this><h3 id=fuck-i-dont-see-any-actual-simulation-methods-in-this><span class=hanchor arialabel=Anchor># </span>Fuck I don’t see any actual simulation methods in this</h3></a><p>Didn’t Yin directly say there were?
Going back to Yin</p><a href=#xutimingintervalsusing2016w-xu-sn-baker-2016-apparently-offers-another-variation-but-with-stdp><h2 id=xutimingintervalsusing2016w-xu-sn-baker-2016-apparently-offers-another-variation-but-with-stdp><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">W. Xu, S.N. Baker (2016)</a> Apparently offers another variation but with STDP</h2></a><ul><li>This appears to use the pacemaker model though</li><li>nvm they claim to use a “beat-frequency” model</li></ul><a href=#reading-from-yinoscillationcoincidencedetectionmodels2022yin-et-al-2022-again><h2 id=reading-from-yinoscillationcoincidencedetectionmodels2022yin-et-al-2022-again><span class=hanchor arialabel=Anchor># </span>Reading from <a class="internal-link broken">Yin et al. (2022)</a> again</h2></a><p>From sec. 3.1:</p><blockquote><p>In the discussion of how their beat generation model fits in with other timing models, Bose et al. (2019) described the features of the SBF model that make it distinctive and set it apart from other models of interval timing. In essence, the SBF model uses neuronal oscillators with different fixed frequencies in an unusual way (see Miall, 1989).</p><p>All the oscillators reset at t = 0; and the differences in the drifting frequencies of convergent units will eventually result in a near-coincidence (the so-called ‘beating phenomenon’ of non-identical oscillators) at a duration that as a result of reinforcement learning (strengthening of synapses onto coincidence-detector units) can match the target duration (Petter et al., 2018).</p><p>As Bose et al. (2019) note, the uniqueness and applicability of this model have not been lost on others as it has been extended to the periodic case and considered for rhythmic prediction/reproduction by a number of investigators interested in the calculation of rate, time, and numbers due to it offering a more general case than the pacemaker–accumulator account of the phenomena (e.g., Brighouse et al., 2014; Hartcher-O’Brien et al., 2016; Teki et al., 2012 – in addition to Gu et al., 2011).</p></blockquote><ul><li>Checking out the <a class="internal-link broken">C. Miall (1989)</a> ref real quick<ul><li>Looks like the basis of the entire model</li></ul></li><li>Also checked out <a class="internal-link broken">Bose et al. (2019)</a> and it does not appear relevant</li></ul><a href=#did-i-not-check-guoscillatorymultiplexingneural2015a><h2 id=did-i-not-check-guoscillatorymultiplexingneural2015a><span class=hanchor arialabel=Anchor># </span>Did I not check <a class="internal-link broken">@guOscillatoryMultiplexingNeural2015a</a></h2></a><ul><li><p>Ok I did a long time ago check <a class="internal-link broken">Outline of SBF Model</a></p><ul><li>Goes over the rhythms in depth</li><li>especially the role of Beta Rhythm</li><li>Appears to implicate a beta and gamma entrainment / phase locking</li></ul></li><li><p>This is going places</p></li><li><p>Particularly wants to point out the implication of working memory within the same structures (which ones?)</p><ul><li>Basal Ganglia duh</li><li>But the other papers including Teki and Yin also want to point out overlaps</li></ul><blockquote><p>As one of the models explaining neuroanatomical localization of working memory, the prefrontal cortex, basal ganglia working memory (PBWM) model (Frank et al., 2001; Hazy et al., 2006, 2007; O’Reilly and Frank, 2006) suggests a critical role for cortico-striatal circuits in selecting and maintaining relevant information in working memory. The PBWM model explains that PFC actively maintains task-relevant information that is dynamically gated/updated by the basal ganglia. In addition, the posterior cortex and hippocampus play a role in automatic sensory/motor processing and the rapid learning of arbitrary associations, respectively (e.g., Collins and Franck, 2012).</p></blockquote><blockquote><p>The proposed neural mechanisms of the PBWM model share some similarities with the SBF model of interval timing. For example, both rely on the posited involvement of the same brain areas—emphasizing a role for the striatum in detecting/gating cortical inputs. Specifically, the role of the striatum has been suggested as detecting the coincident pattern of cortical inputs in the SBF model and as gating/updating information by integration of cortical input and DA signals in PBWM model.</p></blockquote><blockquote><p>The selected (gated) signals are hypothesized to pass through the thalamus to the cortex in both models and to be modulated by DA (see Hazy et al., 2006, 2007; Matell and Meck, 2004). Also, both models accommodate a role for DA signaling in learning so that the synapses of MSNs in the striatum can be weighted appropriately. In the SBF model, feedback and/or the delivery of reward for responses occurring just after the target duration induces phasic DA input to the striatum which can strengthen the synapses of MSN receiving inputs from the relevant subset of cortical oscillating neurons in order for them to serve as “detectors” for specific target durations (Ullsperger et al., 2014; van Rijn et al., 2014). Similarly, the PBWM model explains that phasic DA input modulates the MSN synapses so that the relevant cortical inputs can trigger the gating/updating of the relevant information. Given these similarities, it has been suggested that interval timing and working memory rely not only on the same gross anatomical structures, but also on the same neural representations (Buhusi and Meck, 2009; Lewis and Miall, 2006; Lustig and Meck, 2005; Lustig et al., 2005).</p></blockquote></li></ul><a href=#guoscillatorymultiplexingneural2015agu-et-al-2015-on-the-sbf><h4 id=guoscillatorymultiplexingneural2015agu-et-al-2015-on-the-sbf><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">Gu et al. (2015)</a> on the SBF</h4></a><p>Probably the best anatomical descriptor so far
Move to the citation page</p><blockquote><p>Specifically, the SBF model suggests that each cortical neuron oscillates at a preferred oscillatory frequency covering, for example, the alpha and theta frequency bands. With the onset of a stimulus to-be-timed, the phases of multiple oscillators are reset by a burst of dopaminergic input from the ventral tegmental area (VTA).</p></blockquote><blockquote><p>Then, these cortical neurons continue to oscillate according to their endogenous oscillatory periods, and their coincident activation pattern can be detected by striatal MSNs during the course of the to-be-timed signal as illustrated in Fig. 2.</p></blockquote><blockquote><p>MSN’s have the potential to serve as coincidence detectors because one MSN receives tens of thousands of inputs from divergent cortical and thalamic neurons and needs simultaneous input to be activated (Groves et al., 1995; Wilson, 1995, 1998).</p></blockquote><blockquote><p>The synaptic weights between a MSN and cortical neurons with different endogenous oscillatory periods determine for which duration this MSN encodes. Even with drift in the oscillatory periods over time, the synchrony provided by the resetting at the start of a signal is sufficient to maintain a stable encoding of duration.</p></blockquote><blockquote><p>The combination of drift on the one hand, and the reliance on slower oscillating cortical neurons for MSNs that encode for longer durations provide a constant coefficient of variation across signal durations in the seconds-to-minutes range and match the level of sensitivity to time observed in humans and other animals (Gibbon et al., 1984, 1997; Matell and Meck, 2004; Penney et al., 2008).</p></blockquote><blockquote><p>According to this account, the learning of a new target duration can be explained within the same cortico-striatal circuit by modulating synaptic weights among MSNs and subsets of cortical neurons. For example, if the learning of a 6-s target duration has induced a MSN to be highly connected with a subset of cortical neurons whose firing rates are maximal 6 s after signal onset, the learning of a 10-s target duration will induce a different set of synaptic weights to a MSN as a result of stronger connections with a subset of cortical neurons that fire more frequently around 10 s.</p></blockquote><blockquote><p>Phasic DA release into the striatum from the SNc is hypothesized to serve as the reinforcement signal for learning of new target durations, thus allowing modulation of the MSN synaptic weights and new learning (Agostino et al., 2011; MacDonald et al., 2012; Matell and Meck, 2004).</p></blockquote><ul><li>#dopamine #reinforcement #rl</li></ul><blockquote><p>The strength of the SBF model lies in its specification of known neural mechanisms, as well as its consistency with the available anatomical, behavioral, and pharmacological evidence (Allman and Meck, 2012; Coull et al., 2011; Merchant et al., 2013a). MSNs in the striatum have the appropriate characteristics to serve as a largescale coincidence-detector system because they receive a great deal of convergent, multi-modal input from the cortex (Wilson, 1995, 1998) and such coincident excitatory input from the cortex can drive the MSNs into the “Up state” (O’Donnell and Grace, 1995; Wilson, 1993).</p></blockquote><p>Lists inhibition and lesion induced experiments if I need that. Also other evidence.</p><a href=#variance><h5 id=variance><span class=hanchor arialabel=Anchor># </span>Variance</h5></a><blockquote><p>It has been suggested, however, that this model can be susceptible to the variance of the oscillation periods. If the cortical oscillators are independent of each other so that they have different variations in their oscillation period, the ability of representing detectable coincident patterns will be significantly impaired even with a small amount of variance if they aren’t reset properly.</p><p>The variance in oscillation period (i.e., variability in clock speed) that the model allows under this condition are quite small, i.e., less than 3% of the period, and considering that the variance can accumulate with time, the detection of longer durations in the minutes range could be much more debilitated.</p><p>However, if the variance is introduced globally (e.g., all oscillation periods are increased by 4%), the models’ detection ability is not affected, but only the overall clock speed will be changed (see Oprisan and Buhusi, 2011, 2013, 2014)</p></blockquote><ul><li>what does overall clock speed change mean?</li></ul><blockquote><p>Therefore, additional coupling mechanisms for cortical oscillators that can reduce the independence of oscillating neurons with different frequencies be investigated in order to incorporate a more biologically plausible level of variance into the system while still allowing for independence of multiple timing processes (Buhusi and Meck, 2009b)</p></blockquote><a href=#200623><h1 id=200623><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">20.06.23</a></h1></a><a href=#picking-back-up-from-yesterday><h2 id=picking-back-up-from-yesterday><span class=hanchor arialabel=Anchor># </span>Picking back up from yesterday</h2></a><a href=#reading-hardyneurocomputationalmodelsinterval2016><h3 id=reading-hardyneurocomputationalmodelsinterval2016><span class=hanchor arialabel=Anchor># </span>Reading <a class="internal-link broken">@hardyNeurocomputationalModelsInterval2016</a></h3></a><a href=#also-it-appears-that-the-mosers-have-a-similar-paper><h2 id=also-it-appears-that-the-mosers-have-a-similar-paper><span class=hanchor arialabel=Anchor># </span>Also it appears that the Mosers have a similar paper</h2></a><p><a class="internal-link broken">@tsaoNeuralBasesTiming2022</a>
That references GM
<a href=https://www.nature.com/articles/s41583-022-00623-3 rel=noopener>The neural bases for timing of durations | Nature Reviews Neuroscience</a>
Unfortunately, I can’t find a copy</p><a href=#i-want-to-know-how-they-were-simulated-tekipersistencememoryhow2017><h2 id=i-want-to-know-how-they-were-simulated-tekipersistencememoryhow2017><span class=hanchor arialabel=Anchor># </span>I want to know how they were simulated <a class="internal-link broken">@tekiPersistenceMemoryHow2017</a></h2></a><a href=#other-reading-i-want><h2 id=other-reading-i-want><span class=hanchor arialabel=Anchor># </span>Other reading I want:</h2></a><p><a class="internal-link broken">@tekiPersistenceMemoryHow2017</a>
<a class="internal-link broken">Xu and Baker (2016)</a></p><a href=#buzsakibrainrhythmshave2023><h2 id=buzsakibrainrhythmshave2023><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">@buzsakiBrainRhythmsHave2023</a></h2></a><ul><li>Why neuronal oscillations are important to begin with</li><li>Relating moments of integration to neuronal oscillatory cycles can effectively delineate circuits which are separated in time but still feed to the same downstream neuron<ul><li>I.e. The informational content of the readout of a neuron can be drastically different depending on the temporal context</li></ul></li><li><strong>Mention that oscillations are built on inhibition</strong><ul><li><strong>So using the zeroth-oscillator as an inhibitory circuit</strong></li></ul></li><li>I want to use a timing model that builds on oscillations</li></ul><a href=#maybe-use-a-power-law-distribution-for-the-oscillators-dev><h2 id=maybe-use-a-power-law-distribution-for-the-oscillators-dev><span class=hanchor arialabel=Anchor># </span>Maybe use a power-law distribution for the oscillators? #dev</h2></a><p>Because they mention adding “<em><strong>1/f Noise</strong></em>” somewhere. find that.</p><p>So for closer to RP p(x=RP) = low, x is the oscillator size. Thus P(RP) = 0, because we never want to get it exactly.</p><p>Need to be able to set the minimum and the max = RP</p><a href=#in-python><h3 id=in-python><span class=hanchor arialabel=Anchor># </span>In Python</h3></a><p><a href=https://numpy.org/doc/stable/reference/random/generated/numpy.random.power.html rel=noopener>numpy.random.power — NumPy v1.25 Manual</a>
From
<a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.powerlaw.html rel=noopener>scipy.stats.powerlaw — SciPy v1.10.1 Manual</a>:</p><blockquote><p>The probability density function for powerlaw is:
$$
f(x, a)=a x^{a-1}
$$
for $0 \leq x \leq 1, a>0$
powerlaw takes a as a shape parameter for $a$.
The probability density above is defined in the “standardized” form. To shift and/or scale the distribution use the <code>loc</code> and <code>scale</code> parameters. Specifically, <code>powerlaw.pdf(x, a, loc, scale)</code> is identically equivalent to <code>powerlaw.pdf(y, a) / scale</code> with <code>y = (x - loc) / scale</code>. Note that shifting the location of a distribution does not make it a “noncentral” distribution; noncentral generalizations of some distributions are available in separate classes.</p><p>For example, the support of 
<a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.powerlaw.html#scipy.stats.powerlaw rel=noopener><code>powerlaw</code></a> can be adjusted from the default interval <code>[0, 1]</code> to the interval <code>[c, c+d]</code> by setting <code>loc=c</code> and <code>scale=d</code>. For a power-law distribution with infinite support, see 
<a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pareto.html#scipy.stats.pareto rel=noopener><code>pareto</code></a>.</p></blockquote><a href=#mathematically><h3 id=mathematically><span class=hanchor arialabel=Anchor># </span>Mathematically:</h3></a><p><a href=https://stats.stackexchange.com/questions/88496/accurately-generating-variates-from-discrete-power-law-distribution rel=noopener>random generation - Accurately generating variates from discrete power law distribution - Cross Validated</a>
<a href=https://stats.stackexchange.com/questions/475566/sampling-distribution-of-the-mean-of-the-discrete-power-law-distribution rel=noopener>Sampling distribution of the mean of the discrete-power law distribution - Cross Validated</a></p><p><a href=https://indico.ictp.it/event/a08182/session/44/contribution/28/material/0/0.pdf rel=noopener>ON THE POWER LAW STATISTICAL DISTRIBUTION OF OBSERVATIONS</a>
<a class="internal-link broken">image-20230621192809301.png</a></p><p><a class="internal-link broken">image-20230621193405516.png</a></p><a href=#cold-storage><h2 id=cold-storage><span class=hanchor arialabel=Anchor># </span>Cold Storage</h2></a><ul><li>Most models rely on a cold storage or the “saving to memory slots” of information, memory, and episodes.</li><li>Even assuming some dedicated memory circuit, these must be used to store multiple memory states, and not just one</li><li>Instead memory is distributed across the state of a network or circuit</li></ul><p>See <a class="internal-link broken">Teki et al. (2017)</a> <em><strong>Models of Working Memory</strong></em> Section</p><a href=#working-memory><h2 id=working-memory><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">Working Memory</a></h2></a><blockquote><p>Working memory is traditionally defined as a cognitive capacity for transiently storing, processing, and manipulating information.</p><ul><li><a class="internal-link broken">@tekiPersistenceMemoryHow2017</a></li></ul></blockquote><a href=#190623><h1 id=190623><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">19.06.23</a></h1></a><a href=#extractions-from-reading><h2 id=extractions-from-reading><span class=hanchor arialabel=Anchor># </span>Extractions from reading</h2></a><blockquote><p>Hardy & Buonomano (2016) have recently proposed that the brain encodes time in dynamic patterns of neural activity that are referred to as population clocks.</p></blockquote><ul><li>So find out what is meant by that</li></ul><a href=#170623><h1 id=170623><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">17.06.23</a></h1></a><p>Catching up on SBFA writing
Ok so yesterday I found a bunch of research I was previously doing that I thought was relevant</p><p>I left off at <a class="internal-link broken">The Persistence of Memory How the Brain Encodes Time in Memory - 21.11.22.md</a> and <a class="internal-link broken">@tekiPersistenceMemoryHow2017</a>. This is the one that introduces EIO-SBF</p><p>I think I started at finding the old outlines I made of the SBF <a class="internal-link broken">Outline of SBF Model</a> and <a class="internal-link broken">Outline of SBF Model for Supervision Meeting</a> which have a bit of detail and can absolutely be used for part of my writing.</p><p>But from there I migrated and rediscovered much of the SBF papers that must be referenced.</p><p>Gustavos paper must be included <a class="internal-link broken">@melloNeuralBehavioralMechanisms2016</a></p><a href=#120623><h1 id=120623><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">12.06.23</a></h1></a><p><a class="internal-link broken">Outline 2.0</a></p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/notes/old_index_old/ data-ctx="notes/My page" data-src=/notes/old_index_old class=internal-link>🪴 Quartz 3.3asaasasfs</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://quartz.tarlton.info/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Michael Tarlton using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://quartz.tarlton.info/>Home</a></li><li><a href=https://twitter.com/michatarlton>Twitter</a></li><li><a href=https://github.com/michatarlton>GitHub</a></li></ul></footer></div></div></body></html>